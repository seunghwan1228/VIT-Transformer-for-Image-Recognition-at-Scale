{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "visual_transformer_annatopy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPTaku2VHz0U3pFCw6dYfzN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghwan1228/VIT-Transformer-for-Image-Recognition-at-Scale/blob/main/visual_transformer_annatopy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ-i0MC5_qLc",
        "outputId": "a65921cd-484c-471e-ca41-487ce37fbc63"
      },
      "source": [
        "!git clone https://github.com/seunghwan1228/VIT-Transformer-for-Image-Recognition-at-Scale.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'VIT-Transformer-for-Image-Recognition-at-Scale'...\n",
            "remote: Enumerating objects: 179, done.\u001b[K\n",
            "remote: Counting objects: 100% (179/179), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 179 (delta 93), reused 157 (delta 74), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (179/179), 32.60 KiB | 953.00 KiB/s, done.\n",
            "Resolving deltas: 100% (93/93), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XpahEaJ_wKC",
        "outputId": "153d4c2b-42fc-4de1-b3a2-f5e6aa2c6276"
      },
      "source": [
        "%cd /content/VIT-Transformer-for-Image-Recognition-at-Scale"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/VIT-Transformer-for-Image-Recognition-at-Scale\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZnZ12WtAl16",
        "outputId": "852d968e-90e3-4ff9-baea-f90bf850a62e"
      },
      "source": [
        "!pip install -q tensorflow_addons\n",
        "!pip install -q ruamel.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 706kB 15.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 14.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 552kB 28.5MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-Sji8pMy_zOc",
        "outputId": "5add256e-5bf4-4f7e-9e00-3dddf2a4a3dd"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.view('/content/VIT-Transformer-for-Image-Recognition-at-Scale/model/resnet.py')\n",
        "files.view('/content/VIT-Transformer-for-Image-Recognition-at-Scale/model/transformer.py')\n",
        "files.view('/content/VIT-Transformer-for-Image-Recognition-at-Scale/train_model.py')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/VIT-Transformer-for-Image-Recognition-at-Scale/model/resnet.py\")"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/VIT-Transformer-for-Image-Recognition-at-Scale/model/transformer.py\")"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/VIT-Transformer-for-Image-Recognition-at-Scale/train_model.py\")"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuzLBlS0AFmh",
        "outputId": "a4ec50e4-6524-4b41-f390-078d2ce3c69f"
      },
      "source": [
        "!python train_model.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-02 07:54:47.802764: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-03-02 07:54:50.142912: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-02 07:54:50.143867: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-03-02 07:54:50.176606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-02 07:54:50.177232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-03-02 07:54:50.177269: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-03-02 07:54:50.179258: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
            "2021-03-02 07:54:50.179329: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
            "2021-03-02 07:54:50.181003: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-03-02 07:54:50.181365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-03-02 07:54:50.183269: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-03-02 07:54:50.184468: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-03-02 07:54:50.188158: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-03-02 07:54:50.188304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-02 07:54:50.188962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-02 07:54:50.189535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-03-02 07:54:50.189988: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-03-02 07:54:50.190171: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-02 07:54:50.190272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-02 07:54:50.190867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-03-02 07:54:50.190896: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-03-02 07:54:50.190924: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
            "2021-03-02 07:54:50.190942: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
            "2021-03-02 07:54:50.190962: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-03-02 07:54:50.190982: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-03-02 07:54:50.191001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-03-02 07:54:50.191019: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-03-02 07:54:50.191038: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-03-02 07:54:50.191102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-02 07:54:50.191739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-02 07:54:50.192334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-03-02 07:54:50.192376: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-03-02 07:54:50.703799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-03-02 07:54:50.703850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-03-02 07:54:50.703863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-03-02 07:54:50.704061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-02 07:54:50.704694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-02 07:54:50.705264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-02 07:54:50.705880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14788 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
            "1 Physical Gpu 1 Logical Gpu\n",
            "The Data has 2\n",
            "\n",
            "\n",
            "Preparing Validation Dataset to evaluate\n",
            "\n",
            "\n",
            "Training logs are stored at model_ckpt/vit_20210302/logs/train_data/20210302-075451\n",
            "  0% 0/100 [00:00<?, ?it/s]\n",
            "\n",
            "Start Training 1\n",
            "\n",
            "\n",
            "2021-03-02 07:54:51.655126: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-03-02 07:54:51.655659: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000155000 Hz\n",
            "2021-03-02 07:55:23.542068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
            "2021-03-02 07:55:23.865669: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
            "Train | Step:: 10 - Loss:4.669632434844971 Acc:0.0078125\n",
            "Train | Step:: 20 - Loss:4.65733003616333 Acc:0.010546875186264515\n",
            "Train | Step:: 30 - Loss:4.646580219268799 Acc:0.01145833358168602\n",
            "Train | Step:: 40 - Loss:4.640707969665527 Acc:0.01210937462747097\n",
            "Train | Step:: 50 - Loss:4.636204719543457 Acc:0.012656250037252903\n",
            "Train | Step:: 60 - Loss:4.6328606605529785 Acc:0.012369791977107525\n",
            "Train | Step:: 70 - Loss:4.627781391143799 Acc:0.01294642873108387\n",
            "Train | Step:: 80 - Loss:4.623312950134277 Acc:0.01376953162252903\n",
            "Train | Step:: 90 - Loss:4.618990421295166 Acc:0.01423611119389534\n",
            "Train | Step:: 100 - Loss:4.613212585449219 Acc:0.015156250447034836\n",
            "Train | Step:: 110 - Loss:4.607001781463623 Acc:0.016335226595401764\n",
            "Train | Step:: 120 - Loss:4.600543022155762 Acc:0.01764323003590107\n",
            "Train | Step:: 130 - Loss:4.592228412628174 Acc:0.019170673564076424\n",
            "Train | Step:: 140 - Loss:4.5848870277404785 Acc:0.02047991007566452\n",
            "Train | Step:: 150 - Loss:4.574679374694824 Acc:0.022239582613110542\n",
            "Train | Step:: 160 - Loss:4.565436840057373 Acc:0.02294921875\n",
            "Train | Step:: 170 - Loss:4.55584716796875 Acc:0.023943014442920685\n",
            "Train | Step:: 180 - Loss:4.545060634613037 Acc:0.02595486119389534\n",
            "Train | Step:: 190 - Loss:4.532418251037598 Acc:0.027919407933950424\n",
            "Train | Step:: 200 - Loss:4.5213141441345215 Acc:0.029374999925494194\n",
            "Train | Step:: 210 - Loss:4.511486530303955 Acc:0.0314360111951828\n",
            "Train | Step:: 220 - Loss:4.4997029304504395 Acc:0.03259943053126335\n",
            "Train | Step:: 230 - Loss:4.488239765167236 Acc:0.03393342345952988\n",
            "Train | Step:: 240 - Loss:4.47756814956665 Acc:0.03538411483168602\n",
            "Train | Step:: 250 - Loss:4.467047691345215 Acc:0.03662500157952309\n",
            "Train | Step:: 260 - Loss:4.457079887390137 Acc:0.0375901460647583\n",
            "Train | Step:: 270 - Loss:4.446011543273926 Acc:0.038831017911434174\n",
            "Train | Step:: 280 - Loss:4.435916423797607 Acc:0.04012276604771614\n",
            "Train | Step:: 290 - Loss:4.424106121063232 Acc:0.0414331890642643\n",
            "Train | Step:: 300 - Loss:4.414623737335205 Acc:0.042526040226221085\n",
            "Train | Step:: 310 - Loss:4.404269695281982 Acc:0.043674394488334656\n",
            "Train | Step:: 320 - Loss:4.397071361541748 Acc:0.04453124850988388\n",
            "Train | Step:: 330 - Loss:4.385806560516357 Acc:0.04578598588705063\n",
            "Train | Step:: 340 - Loss:4.375010967254639 Acc:0.047242648899555206\n",
            "Train | Step:: 350 - Loss:4.365013599395752 Acc:0.04861607030034065\n",
            "Train | Step:: 360 - Loss:4.355724811553955 Acc:0.0496961809694767\n",
            "Train | Step:: 370 - Loss:4.344910621643066 Acc:0.05109797418117523\n",
            "Train | Step:: 380 - Loss:4.335958957672119 Acc:0.051994241774082184\n",
            "Train | Step:: 390 - Loss:4.326635837554932 Acc:0.052964743226766586\n",
            "Train | 1 - Loss:4.326635837554932 Acc:0.052964743226766586\n",
            "Valid | 1 - Loss:3.9929778575897217 Acc:0.10116185992956161\n",
            "Time Taken:: 1: 219.18\n",
            "Model Saved at model_ckpt/vit/ckpt-1\n",
            "  1% 1/100 [03:40<6:03:17, 220.18s/it]\n",
            "\n",
            "Start Training 2\n",
            "\n",
            "\n",
            "Train | Step:: 400 - Loss:3.973111629486084 Acc:0.09296874701976776\n",
            "Train | Step:: 410 - Loss:3.9597086906433105 Acc:0.09375\n",
            "Train | Step:: 420 - Loss:3.9639410972595215 Acc:0.09609375149011612\n",
            "Train | Step:: 430 - Loss:3.966601848602295 Acc:0.09824218600988388\n",
            "Train | Step:: 440 - Loss:3.955814838409424 Acc:0.09875000268220901\n",
            "Train | Step:: 450 - Loss:3.95507550239563 Acc:0.10052083432674408\n",
            "Train | Step:: 460 - Loss:3.943376302719116 Acc:0.10189732164144516\n",
            "Train | Step:: 470 - Loss:3.9389195442199707 Acc:0.10361327975988388\n",
            "Train | Step:: 480 - Loss:3.9387471675872803 Acc:0.1032986119389534\n",
            "Train | Step:: 490 - Loss:3.933048725128174 Acc:0.10445312410593033\n",
            "Train | Step:: 500 - Loss:3.9249770641326904 Acc:0.10532670468091965\n",
            "Train | Step:: 510 - Loss:3.9260940551757812 Acc:0.10553385317325592\n",
            "Train | Step:: 520 - Loss:3.9244117736816406 Acc:0.10576923191547394\n",
            "Train | Step:: 530 - Loss:3.918560266494751 Acc:0.10625000298023224\n",
            "Train | Step:: 540 - Loss:3.911531686782837 Acc:0.10739583522081375\n",
            "Train | Step:: 550 - Loss:3.908318281173706 Acc:0.10732422024011612\n",
            "Train | Step:: 560 - Loss:3.8981359004974365 Acc:0.10845588147640228\n",
            "Train | Step:: 570 - Loss:3.8894739151000977 Acc:0.10933160036802292\n",
            "Train | Step:: 580 - Loss:3.881065845489502 Acc:0.11077302694320679\n",
            "Train | Step:: 590 - Loss:3.8748319149017334 Acc:0.1120312511920929\n",
            "Train | Step:: 600 - Loss:3.8719491958618164 Acc:0.11212797462940216\n",
            "Train | Step:: 610 - Loss:3.869368314743042 Acc:0.11278408765792847\n",
            "Train | Step:: 620 - Loss:3.8650240898132324 Acc:0.11294157803058624\n",
            "Train | Step:: 630 - Loss:3.8600094318389893 Acc:0.11402995139360428\n",
            "Train | Step:: 640 - Loss:3.853332042694092 Acc:0.11484374850988388\n",
            "Train | Step:: 650 - Loss:3.846156120300293 Acc:0.11565504968166351\n",
            "Train | Step:: 660 - Loss:3.8387820720672607 Acc:0.11678240448236465\n",
            "Train | Step:: 670 - Loss:3.835087776184082 Acc:0.11735490709543228\n",
            "Train | Step:: 680 - Loss:3.830249071121216 Acc:0.11799568682909012\n",
            "Train | Step:: 690 - Loss:3.8289542198181152 Acc:0.11874999850988388\n",
            "Train | Step:: 700 - Loss:3.8235080242156982 Acc:0.11980846524238586\n",
            "Train | Step:: 710 - Loss:3.8194336891174316 Acc:0.12033691257238388\n",
            "Train | Step:: 720 - Loss:3.8125126361846924 Acc:0.12170927971601486\n",
            "Train | Step:: 730 - Loss:3.807873487472534 Acc:0.12263327091932297\n",
            "Train | Step:: 740 - Loss:3.8042571544647217 Acc:0.12292410433292389\n",
            "Train | Step:: 750 - Loss:3.799710512161255 Acc:0.12350260466337204\n",
            "Train | Step:: 760 - Loss:3.795804738998413 Acc:0.12419763207435608\n",
            "Train | Step:: 770 - Loss:3.791846990585327 Acc:0.1248355284333229\n",
            "Train | Step:: 780 - Loss:3.7866897583007812 Acc:0.12572115659713745\n",
            "Train | 2 - Loss:3.7866897583007812 Acc:0.12572115659713745\n",
            "Valid | 2 - Loss:3.6623101234436035 Acc:0.1573517620563507\n",
            "Time Taken:: 2: 169.75\n",
            "Model Saved at model_ckpt/vit/ckpt-2\n",
            "  2% 2/100 [06:31<5:35:30, 205.41s/it]\n",
            "\n",
            "Start Training 3\n",
            "\n",
            "\n",
            "Train | Step:: 790 - Loss:3.652346134185791 Acc:0.15625\n",
            "Train | Step:: 800 - Loss:3.603239059448242 Acc:0.15937499701976776\n",
            "Train | Step:: 810 - Loss:3.5957040786743164 Acc:0.16067709028720856\n",
            "Train | Step:: 820 - Loss:3.600677967071533 Acc:0.1572265625\n",
            "Train | Step:: 830 - Loss:3.590132236480713 Acc:0.15734374523162842\n",
            "Train | Step:: 840 - Loss:3.5991053581237793 Acc:0.15729166567325592\n",
            "Train | Step:: 850 - Loss:3.5880918502807617 Acc:0.1584821492433548\n",
            "Train | Step:: 860 - Loss:3.588991165161133 Acc:0.15927734971046448\n",
            "Train | Step:: 870 - Loss:3.586940288543701 Acc:0.15963540971279144\n",
            "Train | Step:: 880 - Loss:3.583707332611084 Acc:0.16078124940395355\n",
            "Train | Step:: 890 - Loss:3.576507091522217 Acc:0.16150568425655365\n",
            "Train | Step:: 900 - Loss:3.5779871940612793 Acc:0.16171875596046448\n",
            "Train | Step:: 910 - Loss:3.5774166584014893 Acc:0.16183894872665405\n",
            "Train | Step:: 920 - Loss:3.57456374168396 Acc:0.16188615560531616\n",
            "Train | Step:: 930 - Loss:3.568734884262085 Acc:0.16302083432674408\n",
            "Train | Step:: 940 - Loss:3.567370653152466 Acc:0.16279296576976776\n",
            "Train | Step:: 950 - Loss:3.5598437786102295 Acc:0.1644301414489746\n",
            "Train | Step:: 960 - Loss:3.55405855178833 Acc:0.16453993320465088\n",
            "Train | Step:: 970 - Loss:3.5479414463043213 Acc:0.16533717513084412\n",
            "Train | Step:: 980 - Loss:3.5408213138580322 Acc:0.16695313155651093\n",
            "Train | Step:: 990 - Loss:3.5397794246673584 Acc:0.16711309552192688\n",
            "Train | Step:: 1000 - Loss:3.5384621620178223 Acc:0.16715198755264282\n",
            "Train | Step:: 1010 - Loss:3.535360097885132 Acc:0.16732336580753326\n",
            "Train | Step:: 1020 - Loss:3.531489133834839 Acc:0.16822916269302368\n",
            "Train | Step:: 1030 - Loss:3.5263423919677734 Acc:0.16868749260902405\n",
            "Train | Step:: 1040 - Loss:3.5211360454559326 Acc:0.1693810075521469\n",
            "Train | Step:: 1050 - Loss:3.5154144763946533 Acc:0.1698206067085266\n",
            "Train | Step:: 1060 - Loss:3.5149154663085938 Acc:0.17070312798023224\n",
            "Train | Step:: 1070 - Loss:3.511624813079834 Acc:0.1707974076271057\n",
            "Train | Step:: 1080 - Loss:3.510235548019409 Acc:0.17151041328907013\n",
            "Train | Step:: 1090 - Loss:3.5045835971832275 Acc:0.17258064448833466\n",
            "Train | Step:: 1100 - Loss:3.502089738845825 Acc:0.17304687201976776\n",
            "Train | Step:: 1110 - Loss:3.4971871376037598 Acc:0.17379261553287506\n",
            "Train | Step:: 1120 - Loss:3.494269609451294 Acc:0.17426469922065735\n",
            "Train | Step:: 1130 - Loss:3.4908764362335205 Acc:0.17468750476837158\n",
            "Train | Step:: 1140 - Loss:3.487173557281494 Acc:0.17549912631511688\n",
            "Train | Step:: 1150 - Loss:3.4841835498809814 Acc:0.1756756752729416\n",
            "Train | Step:: 1160 - Loss:3.479262113571167 Acc:0.1765008270740509\n",
            "Train | Step:: 1170 - Loss:3.4753036499023438 Acc:0.17744390666484833\n",
            "Train | 3 - Loss:3.4753036499023438 Acc:0.17744390666484833\n",
            "Valid | 3 - Loss:3.428072452545166 Acc:0.19391025602817535\n",
            "Time Taken:: 3: 169.72\n",
            "Model Saved at model_ckpt/vit/ckpt-3\n",
            "  3% 3/100 [09:21<5:15:11, 194.96s/it]\n",
            "\n",
            "Start Training 4\n",
            "\n",
            "\n",
            "Train | Step:: 1180 - Loss:3.4145030975341797 Acc:0.19218750298023224\n",
            "Train | Step:: 1190 - Loss:3.360718250274658 Acc:0.19843749701976776\n",
            "Train | Step:: 1200 - Loss:3.3430910110473633 Acc:0.20000000298023224\n",
            "Train | Step:: 1210 - Loss:3.3418776988983154 Acc:0.20253905653953552\n",
            "Train | Step:: 1220 - Loss:3.340071201324463 Acc:0.2004687488079071\n",
            "Train | Step:: 1230 - Loss:3.3495688438415527 Acc:0.20000000298023224\n",
            "Train | Step:: 1240 - Loss:3.344698190689087 Acc:0.19888393580913544\n",
            "Train | Step:: 1250 - Loss:3.3458590507507324 Acc:0.19941405951976776\n",
            "Train | Step:: 1260 - Loss:3.3508758544921875 Acc:0.20000000298023224\n",
            "Train | Step:: 1270 - Loss:3.356468439102173 Acc:0.19960936903953552\n",
            "Train | Step:: 1280 - Loss:3.3505160808563232 Acc:0.20106534659862518\n",
            "Train | Step:: 1290 - Loss:3.354257345199585 Acc:0.20013020932674408\n",
            "Train | Step:: 1300 - Loss:3.350790500640869 Acc:0.20000000298023224\n",
            "Train | Step:: 1310 - Loss:3.3472981452941895 Acc:0.20027901232242584\n",
            "Train | Step:: 1320 - Loss:3.3461642265319824 Acc:0.20015625655651093\n",
            "Train | Step:: 1330 - Loss:3.3493545055389404 Acc:0.19985350966453552\n",
            "Train | Step:: 1340 - Loss:3.3418726921081543 Acc:0.2006433755159378\n",
            "Train | Step:: 1350 - Loss:3.3381733894348145 Acc:0.2005208283662796\n",
            "Train | Step:: 1360 - Loss:3.334991693496704 Acc:0.2011924386024475\n",
            "Train | Step:: 1370 - Loss:3.328047275543213 Acc:0.20250000059604645\n",
            "Train | Step:: 1380 - Loss:3.3274013996124268 Acc:0.20338541269302368\n",
            "Train | Step:: 1390 - Loss:3.3290860652923584 Acc:0.20308949053287506\n",
            "Train | Step:: 1400 - Loss:3.3259472846984863 Acc:0.2032608687877655\n",
            "Train | Step:: 1410 - Loss:3.3242669105529785 Acc:0.2037760466337204\n",
            "Train | Step:: 1420 - Loss:3.317737579345703 Acc:0.20478124916553497\n",
            "Train | Step:: 1430 - Loss:3.3143818378448486 Acc:0.205078125\n",
            "Train | Step:: 1440 - Loss:3.309473991394043 Acc:0.20590278506278992\n",
            "Train | Step:: 1450 - Loss:3.3090245723724365 Acc:0.20583146810531616\n",
            "Train | Step:: 1460 - Loss:3.306123971939087 Acc:0.20641164481639862\n",
            "Train | Step:: 1470 - Loss:3.3038175106048584 Acc:0.2071875035762787\n",
            "Train | Step:: 1480 - Loss:3.2985286712646484 Acc:0.20778730511665344\n",
            "Train | Step:: 1490 - Loss:3.2964916229248047 Acc:0.2081298828125\n",
            "Train | Step:: 1500 - Loss:3.2931618690490723 Acc:0.20901988446712494\n",
            "Train | Step:: 1510 - Loss:3.290278196334839 Acc:0.20962776243686676\n",
            "Train | Step:: 1520 - Loss:3.2857155799865723 Acc:0.21020089089870453\n",
            "Train | Step:: 1530 - Loss:3.2818706035614014 Acc:0.21080729365348816\n",
            "Train | Step:: 1540 - Loss:3.279269218444824 Acc:0.21119087934494019\n",
            "Train | Step:: 1550 - Loss:3.2766387462615967 Acc:0.21178042888641357\n",
            "Train | Step:: 1560 - Loss:3.2735793590545654 Acc:0.21247996389865875\n",
            "Train | 4 - Loss:3.2735793590545654 Acc:0.21247996389865875\n",
            "Valid | 4 - Loss:3.3575494289398193 Acc:0.20843349397182465\n",
            "Time Taken:: 4: 169.63\n",
            "Model Saved at model_ckpt/vit/ckpt-4\n",
            "  4% 4/100 [12:12<5:00:10, 187.61s/it]\n",
            "\n",
            "Start Training 5\n",
            "\n",
            "\n",
            "Train | Step:: 1570 - Loss:3.2216053009033203 Acc:0.21875\n",
            "Train | Step:: 1580 - Loss:3.194493293762207 Acc:0.22421875596046448\n",
            "Train | Step:: 1590 - Loss:3.1716768741607666 Acc:0.22838541865348816\n",
            "Train | Step:: 1600 - Loss:3.164564609527588 Acc:0.23027344048023224\n",
            "Train | Step:: 1610 - Loss:3.166926383972168 Acc:0.23125000298023224\n",
            "Train | Step:: 1620 - Loss:3.1708648204803467 Acc:0.23203125596046448\n",
            "Train | Step:: 1630 - Loss:3.166477680206299 Acc:0.23002232611179352\n",
            "Train | Step:: 1640 - Loss:3.1692237854003906 Acc:0.23173828423023224\n",
            "Train | Step:: 1650 - Loss:3.170915365219116 Acc:0.23255208134651184\n",
            "Train | Step:: 1660 - Loss:3.16995906829834 Acc:0.2329687476158142\n",
            "Train | Step:: 1670 - Loss:3.16231632232666 Acc:0.2355823814868927\n",
            "Train | Step:: 1680 - Loss:3.167691469192505 Acc:0.2340494841337204\n",
            "Train | Step:: 1690 - Loss:3.170731782913208 Acc:0.23401442170143127\n",
            "Train | Step:: 1700 - Loss:3.1664352416992188 Acc:0.2349330335855484\n",
            "Train | Step:: 1710 - Loss:3.1611506938934326 Acc:0.23671874403953552\n",
            "Train | Step:: 1720 - Loss:3.1618504524230957 Acc:0.23603515326976776\n",
            "Train | Step:: 1730 - Loss:3.154073476791382 Acc:0.23782169818878174\n",
            "Train | Step:: 1740 - Loss:3.1512131690979004 Acc:0.23802083730697632\n",
            "Train | Step:: 1750 - Loss:3.148477077484131 Acc:0.23873355984687805\n",
            "Train | Step:: 1760 - Loss:3.1421597003936768 Acc:0.23894530534744263\n",
            "Train | Step:: 1770 - Loss:3.1399106979370117 Acc:0.23932291567325592\n",
            "Train | Step:: 1780 - Loss:3.141634225845337 Acc:0.23906250298023224\n",
            "Train | Step:: 1790 - Loss:3.1383471488952637 Acc:0.2392663061618805\n",
            "Train | Step:: 1800 - Loss:3.1374592781066895 Acc:0.23916015028953552\n",
            "Train | Step:: 1810 - Loss:3.132896900177002 Acc:0.24009375274181366\n",
            "Train | Step:: 1820 - Loss:3.1293370723724365 Acc:0.240234375\n",
            "Train | Step:: 1830 - Loss:3.1232452392578125 Acc:0.2412615716457367\n",
            "Train | Step:: 1840 - Loss:3.1219074726104736 Acc:0.24095982313156128\n",
            "Train | Step:: 1850 - Loss:3.1196088790893555 Acc:0.2415948212146759\n",
            "Train | Step:: 1860 - Loss:3.1172897815704346 Acc:0.24255208671092987\n",
            "Train | Step:: 1870 - Loss:3.1127641201019287 Acc:0.24286794662475586\n",
            "Train | Step:: 1880 - Loss:3.109999656677246 Acc:0.24313965439796448\n",
            "Train | Step:: 1890 - Loss:3.105729103088379 Acc:0.2436789721250534\n",
            "Train | Step:: 1900 - Loss:3.1041765213012695 Acc:0.24388787150382996\n",
            "Train | Step:: 1910 - Loss:3.1008570194244385 Acc:0.24453124403953552\n",
            "Train | Step:: 1920 - Loss:3.098470687866211 Acc:0.24496528506278992\n",
            "Train | Step:: 1930 - Loss:3.0977587699890137 Acc:0.24497465789318085\n",
            "Train | Step:: 1940 - Loss:3.09456205368042 Acc:0.24533306062221527\n",
            "Train | Step:: 1950 - Loss:3.0916898250579834 Acc:0.24579326808452606\n",
            "Train | 5 - Loss:3.0916898250579834 Acc:0.24579326808452606\n",
            "Valid | 5 - Loss:3.2335612773895264 Acc:0.23387420177459717\n",
            "Time Taken:: 5: 169.82\n",
            "Model Saved at model_ckpt/vit/ckpt-5\n",
            "  5% 5/100 [15:02<4:49:00, 182.53s/it]\n",
            "\n",
            "Start Training 6\n",
            "\n",
            "\n",
            "Train | Step:: 1960 - Loss:3.0208675861358643 Acc:0.26093751192092896\n",
            "Train | Step:: 1970 - Loss:2.9858062267303467 Acc:0.265625\n",
            "Train | Step:: 1980 - Loss:2.981929302215576 Acc:0.26484376192092896\n",
            "Train | Step:: 1990 - Loss:2.959585666656494 Acc:0.26738280057907104\n",
            "Train | Step:: 2000 - Loss:2.9667882919311523 Acc:0.26656249165534973\n",
            "Train | Step:: 2010 - Loss:2.975679636001587 Acc:0.26640623807907104\n",
            "Train | Step:: 2020 - Loss:2.978034019470215 Acc:0.2645089328289032\n",
            "Train | Step:: 2030 - Loss:2.98461651802063 Acc:0.2632812559604645\n",
            "Train | Step:: 2040 - Loss:2.9990954399108887 Acc:0.2606770694255829\n",
            "Train | Step:: 2050 - Loss:2.9992101192474365 Acc:0.2596093714237213\n",
            "Train | Step:: 2060 - Loss:2.993858814239502 Acc:0.2614346444606781\n",
            "Train | Step:: 2070 - Loss:3.004685878753662 Acc:0.26054686307907104\n",
            "Train | Step:: 2080 - Loss:3.0076916217803955 Acc:0.2606370151042938\n",
            "Train | Step:: 2090 - Loss:3.003279685974121 Acc:0.2600446343421936\n",
            "Train | Step:: 2100 - Loss:2.9991824626922607 Acc:0.2608333230018616\n",
            "Train | Step:: 2110 - Loss:2.999885082244873 Acc:0.2613769471645355\n",
            "Train | Step:: 2120 - Loss:2.993260145187378 Acc:0.2615808844566345\n",
            "Train | Step:: 2130 - Loss:2.990060329437256 Acc:0.2625434100627899\n",
            "Train | Step:: 2140 - Loss:2.9888997077941895 Acc:0.2629522979259491\n",
            "Train | Step:: 2150 - Loss:2.9864251613616943 Acc:0.26359376311302185\n",
            "Train | Step:: 2160 - Loss:2.98563289642334 Acc:0.2639508843421936\n",
            "Train | Step:: 2170 - Loss:2.988593339920044 Acc:0.2633877694606781\n",
            "Train | Step:: 2180 - Loss:2.9856386184692383 Acc:0.26392662525177\n",
            "Train | Step:: 2190 - Loss:2.9846506118774414 Acc:0.2640950381755829\n",
            "Train | Step:: 2200 - Loss:2.979954719543457 Acc:0.265749990940094\n",
            "Train | Step:: 2210 - Loss:2.9775874614715576 Acc:0.2661057710647583\n",
            "Train | Step:: 2220 - Loss:2.9744038581848145 Acc:0.2662326395511627\n",
            "Train | Step:: 2230 - Loss:2.974337339401245 Acc:0.2662109434604645\n",
            "Train | Step:: 2240 - Loss:2.9727845191955566 Acc:0.2669180929660797\n",
            "Train | Step:: 2250 - Loss:2.9731926918029785 Acc:0.2672395706176758\n",
            "Train | Step:: 2260 - Loss:2.969278573989868 Acc:0.2679687440395355\n",
            "Train | Step:: 2270 - Loss:2.9697515964508057 Acc:0.2682861387729645\n",
            "Train | Step:: 2280 - Loss:2.9668328762054443 Acc:0.2686079442501068\n",
            "Train | Step:: 2290 - Loss:2.965618848800659 Acc:0.2690257430076599\n",
            "Train | Step:: 2300 - Loss:2.9634790420532227 Acc:0.2689955234527588\n",
            "Train | Step:: 2310 - Loss:2.962268352508545 Acc:0.2690538167953491\n",
            "Train | Step:: 2320 - Loss:2.9607975482940674 Acc:0.26936233043670654\n",
            "Train | Step:: 2330 - Loss:2.958080768585205 Acc:0.27014803886413574\n",
            "Train | Step:: 2340 - Loss:2.953878402709961 Acc:0.27115383744239807\n",
            "Train | 6 - Loss:2.953878402709961 Acc:0.27115383744239807\n",
            "Valid | 6 - Loss:3.1724934577941895 Acc:0.23758013546466827\n",
            "Time Taken:: 6: 169.83\n",
            "Model Saved at model_ckpt/vit/ckpt-6\n",
            "  6% 6/100 [17:53<4:40:24, 178.99s/it]\n",
            "\n",
            "Start Training 7\n",
            "\n",
            "\n",
            "Train | Step:: 2350 - Loss:2.8760738372802734 Acc:0.27265626192092896\n",
            "Train | Step:: 2360 - Loss:2.877236843109131 Acc:0.2730468809604645\n",
            "Train | Step:: 2370 - Loss:2.8912899494171143 Acc:0.2747395932674408\n",
            "Train | Step:: 2380 - Loss:2.867854595184326 Acc:0.2816406190395355\n",
            "Train | Step:: 2390 - Loss:2.862222909927368 Acc:0.2837499976158142\n",
            "Train | Step:: 2400 - Loss:2.8610665798187256 Acc:0.28723958134651184\n",
            "Train | Step:: 2410 - Loss:2.8627872467041016 Acc:0.2856026887893677\n",
            "Train | Step:: 2420 - Loss:2.8687620162963867 Acc:0.28730469942092896\n",
            "Train | Step:: 2430 - Loss:2.8790836334228516 Acc:0.28480902314186096\n",
            "Train | Step:: 2440 - Loss:2.8741371631622314 Acc:0.28453123569488525\n",
            "Train | Step:: 2450 - Loss:2.8685100078582764 Acc:0.2865767180919647\n",
            "Train | Step:: 2460 - Loss:2.8740785121917725 Acc:0.2874999940395355\n",
            "Train | Step:: 2470 - Loss:2.876154661178589 Acc:0.2867187559604645\n",
            "Train | Step:: 2480 - Loss:2.8765103816986084 Acc:0.28649553656578064\n",
            "Train | Step:: 2490 - Loss:2.876018762588501 Acc:0.2863541543483734\n",
            "Train | Step:: 2500 - Loss:2.880993127822876 Acc:0.28520506620407104\n",
            "Train | Step:: 2510 - Loss:2.873356819152832 Acc:0.28612130880355835\n",
            "Train | Step:: 2520 - Loss:2.8714375495910645 Acc:0.2869357764720917\n",
            "Train | Step:: 2530 - Loss:2.8715107440948486 Acc:0.28754112124443054\n",
            "Train | Step:: 2540 - Loss:2.870720148086548 Acc:0.28816404938697815\n",
            "Train | Step:: 2550 - Loss:2.8693175315856934 Acc:0.28902530670166016\n",
            "Train | Step:: 2560 - Loss:2.870826244354248 Acc:0.28856533765792847\n",
            "Train | Step:: 2570 - Loss:2.867908239364624 Acc:0.289028525352478\n",
            "Train | Step:: 2580 - Loss:2.868276357650757 Acc:0.28942057490348816\n",
            "Train | Step:: 2590 - Loss:2.862823486328125 Acc:0.2901562452316284\n",
            "Train | Step:: 2600 - Loss:2.85974383354187 Acc:0.29029446840286255\n",
            "Train | Step:: 2610 - Loss:2.8559529781341553 Acc:0.2904224395751953\n",
            "Train | Step:: 2620 - Loss:2.857923984527588 Acc:0.2899274528026581\n",
            "Train | Step:: 2630 - Loss:2.8577072620391846 Acc:0.2895474135875702\n",
            "Train | Step:: 2640 - Loss:2.861062526702881 Acc:0.2893229126930237\n",
            "Train | Step:: 2650 - Loss:2.85811185836792 Acc:0.28974294662475586\n",
            "Train | Step:: 2660 - Loss:2.857260227203369 Acc:0.2899169921875\n",
            "Train | Step:: 2670 - Loss:2.853776454925537 Acc:0.2908143997192383\n",
            "Train | Step:: 2680 - Loss:2.85184907913208 Acc:0.29124540090560913\n",
            "Train | Step:: 2690 - Loss:2.850489616394043 Acc:0.29080358147621155\n",
            "Train | Step:: 2700 - Loss:2.8502190113067627 Acc:0.2907335162162781\n",
            "Train | Step:: 2710 - Loss:2.8484625816345215 Acc:0.2908361554145813\n",
            "Train | Step:: 2720 - Loss:2.8444364070892334 Acc:0.29181742668151855\n",
            "Train | Step:: 2730 - Loss:2.8415818214416504 Acc:0.29258814454078674\n",
            "Train | 7 - Loss:2.8415818214416504 Acc:0.29258814454078674\n",
            "Valid | 7 - Loss:3.2363123893737793 Acc:0.2341746836900711\n",
            "Time Taken:: 7: 169.71\n",
            "Model Saved at model_ckpt/vit/ckpt-7\n",
            "  7% 7/100 [20:44<4:33:31, 176.46s/it]\n",
            "\n",
            "Start Training 8\n",
            "\n",
            "\n",
            "Train | Step:: 2740 - Loss:2.741363525390625 Acc:0.3062500059604645\n",
            "Train | Step:: 2750 - Loss:2.7242980003356934 Acc:0.3140625059604645\n",
            "Train | Step:: 2760 - Loss:2.726300001144409 Acc:0.31197917461395264\n",
            "Train | Step:: 2770 - Loss:2.732149600982666 Acc:0.3121093809604645\n",
            "Train | Step:: 2780 - Loss:2.7381751537323 Acc:0.31296876072883606\n",
            "Train | Step:: 2790 - Loss:2.7465531826019287 Acc:0.3127604126930237\n",
            "Train | Step:: 2800 - Loss:2.7484524250030518 Acc:0.3095982074737549\n",
            "Train | Step:: 2810 - Loss:2.755764961242676 Acc:0.30859375\n",
            "Train | Step:: 2820 - Loss:2.7682087421417236 Acc:0.30633682012557983\n",
            "Train | Step:: 2830 - Loss:2.76780366897583 Acc:0.3062500059604645\n",
            "Train | Step:: 2840 - Loss:2.7647626399993896 Acc:0.3059659004211426\n",
            "Train | Step:: 2850 - Loss:2.769719362258911 Acc:0.30488282442092896\n",
            "Train | Step:: 2860 - Loss:2.772266149520874 Acc:0.3041466474533081\n",
            "Train | Step:: 2870 - Loss:2.771777391433716 Acc:0.30390626192092896\n",
            "Train | Step:: 2880 - Loss:2.7708418369293213 Acc:0.3043749928474426\n",
            "Train | Step:: 2890 - Loss:2.7742063999176025 Acc:0.30473631620407104\n",
            "Train | Step:: 2900 - Loss:2.770474433898926 Acc:0.3045496344566345\n",
            "Train | Step:: 2910 - Loss:2.7696454524993896 Acc:0.3050781190395355\n",
            "Train | Step:: 2920 - Loss:2.7675580978393555 Acc:0.30583882331848145\n",
            "Train | Step:: 2930 - Loss:2.7636470794677734 Acc:0.3057812452316284\n",
            "Train | Step:: 2940 - Loss:2.7641613483428955 Acc:0.3064732253551483\n",
            "Train | Step:: 2950 - Loss:2.765315294265747 Acc:0.3060724437236786\n",
            "Train | Step:: 2960 - Loss:2.763491153717041 Acc:0.3067595064640045\n",
            "Train | Step:: 2970 - Loss:2.767625570297241 Acc:0.3062500059604645\n",
            "Train | Step:: 2980 - Loss:2.763496160507202 Acc:0.3072187602519989\n",
            "Train | Step:: 2990 - Loss:2.7622296810150146 Acc:0.3066105842590332\n",
            "Train | Step:: 3000 - Loss:2.7599799633026123 Acc:0.3067997694015503\n",
            "Train | Step:: 3010 - Loss:2.763806104660034 Acc:0.3066127300262451\n",
            "Train | Step:: 3020 - Loss:2.7655436992645264 Acc:0.3058728575706482\n",
            "Train | Step:: 3030 - Loss:2.7688682079315186 Acc:0.3054427206516266\n",
            "Train | Step:: 3040 - Loss:2.7681009769439697 Acc:0.3057711720466614\n",
            "Train | Step:: 3050 - Loss:2.76967191696167 Acc:0.3052734434604645\n",
            "Train | Step:: 3060 - Loss:2.7674598693847656 Acc:0.305800199508667\n",
            "Train | Step:: 3070 - Loss:2.7657101154327393 Acc:0.3064568042755127\n",
            "Train | Step:: 3080 - Loss:2.7661497592926025 Acc:0.30573660135269165\n",
            "Train | Step:: 3090 - Loss:2.7647457122802734 Acc:0.3055555522441864\n",
            "Train | Step:: 3100 - Loss:2.7636473178863525 Acc:0.3059966266155243\n",
            "Train | Step:: 3110 - Loss:2.760871410369873 Acc:0.3071340322494507\n",
            "Train | Step:: 3120 - Loss:2.760108470916748 Acc:0.3074919879436493\n",
            "Train | 8 - Loss:2.760108470916748 Acc:0.3074919879436493\n",
            "Valid | 8 - Loss:3.259021282196045 Acc:0.23217147588729858\n",
            "Time Taken:: 8: 169.77\n",
            "Model Saved at model_ckpt/vit/ckpt-8\n",
            "  8% 8/100 [23:34<4:27:54, 174.72s/it]\n",
            "\n",
            "Start Training 9\n",
            "\n",
            "\n",
            "Train | Step:: 3130 - Loss:2.748108386993408 Acc:0.3046875\n",
            "Train | Step:: 3140 - Loss:2.681997060775757 Acc:0.32499998807907104\n",
            "Train | Step:: 3150 - Loss:2.690114736557007 Acc:0.32109373807907104\n",
            "Train | Step:: 3160 - Loss:2.696394443511963 Acc:0.3179687559604645\n",
            "Train | Step:: 3170 - Loss:2.7095589637756348 Acc:0.31734374165534973\n",
            "Train | Step:: 3180 - Loss:2.7254388332366943 Acc:0.31302082538604736\n",
            "Train | Step:: 3190 - Loss:2.7228031158447266 Acc:0.3136160671710968\n",
            "Train | Step:: 3200 - Loss:2.715878963470459 Acc:0.31640625\n",
            "Train | Step:: 3210 - Loss:2.721446990966797 Acc:0.3157117962837219\n",
            "Train | Step:: 3220 - Loss:2.718937873840332 Acc:0.3167187571525574\n",
            "Train | Step:: 3230 - Loss:2.7117416858673096 Acc:0.31832385063171387\n",
            "Train | Step:: 3240 - Loss:2.7136034965515137 Acc:0.3179687559604645\n",
            "Train | Step:: 3250 - Loss:2.7159743309020996 Acc:0.3164663314819336\n",
            "Train | Step:: 3260 - Loss:2.7128567695617676 Acc:0.3162946403026581\n",
            "Train | Step:: 3270 - Loss:2.710130453109741 Acc:0.31578123569488525\n",
            "Train | Step:: 3280 - Loss:2.715391159057617 Acc:0.3143066465854645\n",
            "Train | Step:: 3290 - Loss:2.7140753269195557 Acc:0.3142922818660736\n",
            "Train | Step:: 3300 - Loss:2.7148563861846924 Acc:0.31436631083488464\n",
            "Train | Step:: 3310 - Loss:2.717632532119751 Acc:0.31492599844932556\n",
            "Train | Step:: 3320 - Loss:2.7192208766937256 Acc:0.31480467319488525\n",
            "Train | Step:: 3330 - Loss:2.7204511165618896 Acc:0.31495535373687744\n",
            "Train | Step:: 3340 - Loss:2.7224361896514893 Acc:0.31477272510528564\n",
            "Train | Step:: 3350 - Loss:2.721961259841919 Acc:0.31436821818351746\n",
            "Train | Step:: 3360 - Loss:2.721191167831421 Acc:0.31523436307907104\n",
            "Train | Step:: 3370 - Loss:2.718015193939209 Acc:0.3160000145435333\n",
            "Train | Step:: 3380 - Loss:2.7162091732025146 Acc:0.31568509340286255\n",
            "Train | Step:: 3390 - Loss:2.7113986015319824 Acc:0.3162037134170532\n",
            "Train | Step:: 3400 - Loss:2.7119908332824707 Acc:0.3166573643684387\n",
            "Train | Step:: 3410 - Loss:2.7099149227142334 Acc:0.31775322556495667\n",
            "Train | Step:: 3420 - Loss:2.708815336227417 Acc:0.3185677230358124\n",
            "Train | Step:: 3430 - Loss:2.7054989337921143 Acc:0.31955644488334656\n",
            "Train | Step:: 3440 - Loss:2.7054390907287598 Acc:0.3195556700229645\n",
            "Train | Step:: 3450 - Loss:2.7049083709716797 Acc:0.31981533765792847\n",
            "Train | Step:: 3460 - Loss:2.7020115852355957 Acc:0.3204733431339264\n",
            "Train | Step:: 3470 - Loss:2.698601484298706 Acc:0.32091519236564636\n",
            "Train | Step:: 3480 - Loss:2.6965224742889404 Acc:0.3209201395511627\n",
            "Train | Step:: 3490 - Loss:2.696171522140503 Acc:0.32062920928001404\n",
            "Train | Step:: 3500 - Loss:2.6946654319763184 Acc:0.3211554288864136\n",
            "Train | Step:: 3510 - Loss:2.6932332515716553 Acc:0.3210136294364929\n",
            "Train | 9 - Loss:2.6932332515716553 Acc:0.3210136294364929\n",
            "Valid | 9 - Loss:3.273160934448242 Acc:0.24539263546466827\n",
            "Time Taken:: 9: 169.69\n",
            "Model Saved at model_ckpt/vit/ckpt-9\n",
            "  9% 9/100 [26:25<4:23:06, 173.48s/it]\n",
            "\n",
            "Start Training 10\n",
            "\n",
            "\n",
            "Train | Step:: 3520 - Loss:2.7292590141296387 Acc:0.32343751192092896\n",
            "Train | Step:: 3530 - Loss:2.674149990081787 Acc:0.326171875\n",
            "Train | Step:: 3540 - Loss:2.6420633792877197 Acc:0.33281248807907104\n",
            "Train | Step:: 3550 - Loss:2.6399312019348145 Acc:0.33222657442092896\n",
            "Train | Step:: 3560 - Loss:2.6431407928466797 Acc:0.3296875059604645\n",
            "Train | Step:: 3570 - Loss:2.670844316482544 Acc:0.32565104961395264\n",
            "Train | Step:: 3580 - Loss:2.6717097759246826 Acc:0.3241071403026581\n",
            "Train | Step:: 3590 - Loss:2.6700406074523926 Acc:0.3252929747104645\n",
            "Train | Step:: 3600 - Loss:2.6815531253814697 Acc:0.32239583134651184\n",
            "Train | Step:: 3610 - Loss:2.6847503185272217 Acc:0.32249999046325684\n",
            "Train | Step:: 3620 - Loss:2.678151845932007 Acc:0.32343751192092896\n",
            "Train | Step:: 3630 - Loss:2.6822171211242676 Acc:0.3208984434604645\n",
            "Train | Step:: 3640 - Loss:2.6833744049072266 Acc:0.3218750059604645\n",
            "Train | Step:: 3650 - Loss:2.681145191192627 Acc:0.3217075765132904\n",
            "Train | Step:: 3660 - Loss:2.683626413345337 Acc:0.3218750059604645\n",
            "Train | Step:: 3670 - Loss:2.6854796409606934 Acc:0.32133787870407104\n",
            "Train | Step:: 3680 - Loss:2.680196523666382 Acc:0.3226562440395355\n",
            "Train | Step:: 3690 - Loss:2.6771130561828613 Acc:0.32391494512557983\n",
            "Train | Step:: 3700 - Loss:2.6786110401153564 Acc:0.3240542709827423\n",
            "Train | Step:: 3710 - Loss:2.6781115531921387 Acc:0.3248828053474426\n",
            "Train | Step:: 3720 - Loss:2.677825927734375 Acc:0.32462796568870544\n",
            "Train | Step:: 3730 - Loss:2.677640914916992 Acc:0.3249644935131073\n",
            "Train | Step:: 3740 - Loss:2.673774242401123 Acc:0.32550951838493347\n",
            "Train | Step:: 3750 - Loss:2.672409772872925 Acc:0.3267578184604645\n",
            "Train | Step:: 3760 - Loss:2.665651559829712 Acc:0.328187495470047\n",
            "Train | Step:: 3770 - Loss:2.6634750366210938 Acc:0.3282151520252228\n",
            "Train | Step:: 3780 - Loss:2.6625804901123047 Acc:0.32821181416511536\n",
            "Train | Step:: 3790 - Loss:2.663710355758667 Acc:0.32776227593421936\n",
            "Train | Step:: 3800 - Loss:2.663168430328369 Acc:0.32758620381355286\n",
            "Train | Step:: 3810 - Loss:2.663769483566284 Acc:0.32809895277023315\n",
            "Train | Step:: 3820 - Loss:2.6589863300323486 Acc:0.3290574550628662\n",
            "Train | Step:: 3830 - Loss:2.6582510471343994 Acc:0.3297363221645355\n",
            "Train | Step:: 3840 - Loss:2.6572213172912598 Acc:0.32992425560951233\n",
            "Train | Step:: 3850 - Loss:2.6557188034057617 Acc:0.3302619457244873\n",
            "Train | Step:: 3860 - Loss:2.651571750640869 Acc:0.3304687440395355\n",
            "Train | Step:: 3870 - Loss:2.649280071258545 Acc:0.33038192987442017\n",
            "Train | Step:: 3880 - Loss:2.6475670337677 Acc:0.33029982447624207\n",
            "Train | Step:: 3890 - Loss:2.644379138946533 Acc:0.33120888471603394\n",
            "Train | Step:: 3900 - Loss:2.6425044536590576 Acc:0.3312700390815735\n",
            "Train | 10 - Loss:2.6425044536590576 Acc:0.3312700390815735\n",
            "Valid | 10 - Loss:3.2747862339019775 Acc:0.24178685247898102\n",
            "Time Taken:: 10: 169.70\n",
            "Model Saved at model_ckpt/vit/ckpt-10\n",
            " 10% 10/100 [29:16<4:18:56, 172.62s/it]\n",
            "\n",
            "Start Training 11\n",
            "\n",
            "\n",
            "Train | Step:: 3910 - Loss:2.6242001056671143 Acc:0.33203125\n",
            "Train | Step:: 3920 - Loss:2.601675033569336 Acc:0.3402343690395355\n",
            "Train | Step:: 3930 - Loss:2.6033313274383545 Acc:0.3372395932674408\n",
            "Train | Step:: 3940 - Loss:2.5879874229431152 Acc:0.33808594942092896\n",
            "Train | Step:: 3950 - Loss:2.594230890274048 Acc:0.3387500047683716\n",
            "Train | Step:: 3960 - Loss:2.6000287532806396 Acc:0.3381510376930237\n",
            "Train | Step:: 3970 - Loss:2.6035916805267334 Acc:0.33627232909202576\n",
            "Train | Step:: 3980 - Loss:2.6066973209381104 Acc:0.33662110567092896\n",
            "Train | Step:: 3990 - Loss:2.6129703521728516 Acc:0.3372395932674408\n",
            "Train | Step:: 4000 - Loss:2.6045284271240234 Acc:0.3392968773841858\n",
            "Train | Step:: 4010 - Loss:2.5992867946624756 Acc:0.3404119312763214\n",
            "Train | Step:: 4020 - Loss:2.598389148712158 Acc:0.34069010615348816\n",
            "Train | Step:: 4030 - Loss:2.59574556350708 Acc:0.3421874940395355\n",
            "Train | Step:: 4040 - Loss:2.5966062545776367 Acc:0.34185266494750977\n",
            "Train | Step:: 4050 - Loss:2.598698854446411 Acc:0.34182292222976685\n",
            "Train | Step:: 4060 - Loss:2.60217022895813 Acc:0.34101563692092896\n",
            "Train | Step:: 4070 - Loss:2.594909429550171 Acc:0.3421415388584137\n",
            "Train | Step:: 4080 - Loss:2.5884056091308594 Acc:0.34409722685813904\n",
            "Train | Step:: 4090 - Loss:2.590315103530884 Acc:0.343379944562912\n",
            "Train | Step:: 4100 - Loss:2.586470127105713 Acc:0.34375\n",
            "Train | Step:: 4110 - Loss:2.5858192443847656 Acc:0.34371280670166016\n",
            "Train | Step:: 4120 - Loss:2.5850539207458496 Acc:0.3436789810657501\n",
            "Train | Step:: 4130 - Loss:2.580350399017334 Acc:0.3455502688884735\n",
            "Train | Step:: 4140 - Loss:2.581096649169922 Acc:0.3457682430744171\n",
            "Train | Step:: 4150 - Loss:2.5731499195098877 Acc:0.3474999964237213\n",
            "Train | Step:: 4160 - Loss:2.5706546306610107 Acc:0.34750601649284363\n",
            "Train | Step:: 4170 - Loss:2.5664737224578857 Acc:0.34794560074806213\n",
            "Train | Step:: 4180 - Loss:2.5676441192626953 Acc:0.34773996472358704\n",
            "Train | Step:: 4190 - Loss:2.566610813140869 Acc:0.34830281138420105\n",
            "Train | Step:: 4200 - Loss:2.5683019161224365 Acc:0.3484114706516266\n",
            "Train | Step:: 4210 - Loss:2.563675880432129 Acc:0.3494959771633148\n",
            "Train | Step:: 4220 - Loss:2.5639562606811523 Acc:0.3493896424770355\n",
            "Train | Step:: 4230 - Loss:2.5639491081237793 Acc:0.34947916865348816\n",
            "Train | Step:: 4240 - Loss:2.5621132850646973 Acc:0.3499310612678528\n",
            "Train | Step:: 4250 - Loss:2.558912992477417 Acc:0.3503125011920929\n",
            "Train | Step:: 4260 - Loss:2.5548012256622314 Acc:0.35121527314186096\n",
            "Train | Step:: 4270 - Loss:2.551931858062744 Acc:0.35137248039245605\n",
            "Train | Step:: 4280 - Loss:2.5502376556396484 Acc:0.35182976722717285\n",
            "Train | Step:: 4290 - Loss:2.5495195388793945 Acc:0.35146233439445496\n",
            "Train | 11 - Loss:2.5495195388793945 Acc:0.35146233439445496\n",
            "Valid | 11 - Loss:3.23302960395813 Acc:0.24669471383094788\n",
            "Time Taken:: 11: 169.57\n",
            "Model Saved at model_ckpt/vit/ckpt-11\n",
            " 11% 11/100 [32:06<4:15:06, 171.98s/it]\n",
            "\n",
            "Start Training 12\n",
            "\n",
            "\n",
            "Train | Step:: 4300 - Loss:2.5252110958099365 Acc:0.35546875\n",
            "Train | Step:: 4310 - Loss:2.5260651111602783 Acc:0.35429686307907104\n",
            "Train | Step:: 4320 - Loss:2.507354736328125 Acc:0.35624998807907104\n",
            "Train | Step:: 4330 - Loss:2.50892972946167 Acc:0.3525390625\n",
            "Train | Step:: 4340 - Loss:2.509031295776367 Acc:0.35609376430511475\n",
            "Train | Step:: 4350 - Loss:2.509676218032837 Acc:0.3583333194255829\n",
            "Train | Step:: 4360 - Loss:2.513376235961914 Acc:0.3565848171710968\n",
            "Train | Step:: 4370 - Loss:2.519695997238159 Acc:0.35332030057907104\n",
            "Train | Step:: 4380 - Loss:2.5278186798095703 Acc:0.3532986044883728\n",
            "Train | Step:: 4390 - Loss:2.523470640182495 Acc:0.3550781309604645\n",
            "Train | Step:: 4400 - Loss:2.5137856006622314 Acc:0.35738635063171387\n",
            "Train | Step:: 4410 - Loss:2.5088696479797363 Acc:0.35950520634651184\n",
            "Train | Step:: 4420 - Loss:2.50656795501709 Acc:0.3602764308452606\n",
            "Train | Step:: 4430 - Loss:2.5028021335601807 Acc:0.361328125\n",
            "Train | Step:: 4440 - Loss:2.504631519317627 Acc:0.36119791865348816\n",
            "Train | Step:: 4450 - Loss:2.5072813034057617 Acc:0.36079102754592896\n",
            "Train | Step:: 4460 - Loss:2.501058340072632 Acc:0.36250001192092896\n",
            "Train | Step:: 4470 - Loss:2.493694543838501 Acc:0.3645399212837219\n",
            "Train | Step:: 4480 - Loss:2.4967000484466553 Acc:0.36336347460746765\n",
            "Train | Step:: 4490 - Loss:2.4962220191955566 Acc:0.3631640672683716\n",
            "Train | Step:: 4500 - Loss:2.4950809478759766 Acc:0.3638392984867096\n",
            "Train | Step:: 4510 - Loss:2.4923532009124756 Acc:0.3642045557498932\n",
            "Train | Step:: 4520 - Loss:2.487891674041748 Acc:0.36504754424095154\n",
            "Train | Step:: 4530 - Loss:2.4824585914611816 Acc:0.3663736879825592\n",
            "Train | Step:: 4540 - Loss:2.4756436347961426 Acc:0.36778125166893005\n",
            "Train | Step:: 4550 - Loss:2.473592758178711 Acc:0.368028849363327\n",
            "Train | Step:: 4560 - Loss:2.468202590942383 Acc:0.3692997694015503\n",
            "Train | Step:: 4570 - Loss:2.4695005416870117 Acc:0.36919641494750977\n",
            "Train | Step:: 4580 - Loss:2.467026472091675 Acc:0.36974677443504333\n",
            "Train | Step:: 4590 - Loss:2.4649720191955566 Acc:0.3706510365009308\n",
            "Train | Step:: 4600 - Loss:2.4593820571899414 Acc:0.37235382199287415\n",
            "Train | Step:: 4610 - Loss:2.45563006401062 Acc:0.3732666075229645\n",
            "Train | Step:: 4620 - Loss:2.4521517753601074 Acc:0.3741714060306549\n",
            "Train | Step:: 4630 - Loss:2.4508426189422607 Acc:0.374471515417099\n",
            "Train | Step:: 4640 - Loss:2.449223279953003 Acc:0.3745982050895691\n",
            "Train | Step:: 4650 - Loss:2.446381092071533 Acc:0.3747829794883728\n",
            "Train | Step:: 4660 - Loss:2.4444706439971924 Acc:0.3750211000442505\n",
            "Train | Step:: 4670 - Loss:2.4424827098846436 Acc:0.37543174624443054\n",
            "Train | Step:: 4680 - Loss:2.437497615814209 Acc:0.37662258744239807\n",
            "Train | 12 - Loss:2.437497615814209 Acc:0.37662258744239807\n",
            "Valid | 12 - Loss:3.0643482208251953 Acc:0.2784455120563507\n",
            "Time Taken:: 12: 169.69\n",
            "Model Saved at model_ckpt/vit/ckpt-12\n",
            " 12% 12/100 [34:57<4:11:37, 171.56s/it]\n",
            "\n",
            "Start Training 13\n",
            "\n",
            "\n",
            "Train | Step:: 4690 - Loss:2.3255934715270996 Acc:0.3968749940395355\n",
            "Train | Step:: 4700 - Loss:2.351344585418701 Acc:0.38749998807907104\n",
            "Train | Step:: 4710 - Loss:2.332488775253296 Acc:0.3932291567325592\n",
            "Train | Step:: 4720 - Loss:2.334895610809326 Acc:0.3912109434604645\n",
            "Train | Step:: 4730 - Loss:2.3372833728790283 Acc:0.3917187452316284\n",
            "Train | Step:: 4740 - Loss:2.341317892074585 Acc:0.39270833134651184\n",
            "Train | Step:: 4750 - Loss:2.3541531562805176 Acc:0.3895089328289032\n",
            "Train | Step:: 4760 - Loss:2.3558249473571777 Acc:0.3890624940395355\n",
            "Train | Step:: 4770 - Loss:2.357024908065796 Acc:0.38793402910232544\n",
            "Train | Step:: 4780 - Loss:2.352786064147949 Acc:0.38999998569488525\n",
            "Train | Step:: 4790 - Loss:2.344282388687134 Acc:0.39112216234207153\n",
            "Train | Step:: 4800 - Loss:2.338034152984619 Acc:0.3929036557674408\n",
            "Train | Step:: 4810 - Loss:2.3394410610198975 Acc:0.39242789149284363\n",
            "Train | Step:: 4820 - Loss:2.3459866046905518 Acc:0.3918526768684387\n",
            "Train | Step:: 4830 - Loss:2.345106840133667 Acc:0.39276042580604553\n",
            "Train | Step:: 4840 - Loss:2.34706974029541 Acc:0.39208984375\n",
            "Train | Step:: 4850 - Loss:2.3417038917541504 Acc:0.3923713266849518\n",
            "Train | Step:: 4860 - Loss:2.3383736610412598 Acc:0.3941406309604645\n",
            "Train | Step:: 4870 - Loss:2.3432106971740723 Acc:0.3933388292789459\n",
            "Train | Step:: 4880 - Loss:2.345526695251465 Acc:0.39304688572883606\n",
            "Train | Step:: 4890 - Loss:2.3492839336395264 Acc:0.3916666805744171\n",
            "Train | Step:: 4900 - Loss:2.3479621410369873 Acc:0.39236506819725037\n",
            "Train | Step:: 4910 - Loss:2.3423659801483154 Acc:0.39429348707199097\n",
            "Train | Step:: 4920 - Loss:2.339707136154175 Acc:0.39531248807907104\n",
            "Train | Step:: 4930 - Loss:2.332267999649048 Acc:0.39750000834465027\n",
            "Train | Step:: 4940 - Loss:2.327744960784912 Acc:0.39876803755760193\n",
            "Train | Step:: 4950 - Loss:2.323594331741333 Acc:0.39890044927597046\n",
            "Train | Step:: 4960 - Loss:2.327354907989502 Acc:0.39891183376312256\n",
            "Train | Step:: 4970 - Loss:2.326631546020508 Acc:0.39897629618644714\n",
            "Train | Step:: 4980 - Loss:2.3250017166137695 Acc:0.3993229269981384\n",
            "Train | Step:: 4990 - Loss:2.320420742034912 Acc:0.40010079741477966\n",
            "Train | Step:: 5000 - Loss:2.316987991333008 Acc:0.4011474549770355\n",
            "Train | Step:: 5010 - Loss:2.3117172718048096 Acc:0.4024621248245239\n",
            "Train | Step:: 5020 - Loss:2.3105082511901855 Acc:0.4028952121734619\n",
            "Train | Step:: 5030 - Loss:2.3046114444732666 Acc:0.404285728931427\n",
            "Train | Step:: 5040 - Loss:2.300597667694092 Acc:0.4047743082046509\n",
            "Train | Step:: 5050 - Loss:2.2999534606933594 Acc:0.40487754344940186\n",
            "Train | Step:: 5060 - Loss:2.2976977825164795 Acc:0.4057565927505493\n",
            "Train | Step:: 5070 - Loss:2.2936134338378906 Acc:0.40641024708747864\n",
            "Train | 13 - Loss:2.2936134338378906 Acc:0.40641024708747864\n",
            "Valid | 13 - Loss:3.1092615127563477 Acc:0.27333733439445496\n",
            "Time Taken:: 13: 169.85\n",
            "Model Saved at model_ckpt/vit/ckpt-13\n",
            " 13% 13/100 [37:47<4:08:24, 171.32s/it]\n",
            "\n",
            "Start Training 14\n",
            "\n",
            "\n",
            "Train | Step:: 5080 - Loss:2.0975992679595947 Acc:0.4585937559604645\n",
            "Train | Step:: 5090 - Loss:2.202481985092163 Acc:0.4253906309604645\n",
            "Train | Step:: 5100 - Loss:2.204362392425537 Acc:0.42239582538604736\n",
            "Train | Step:: 5110 - Loss:2.1989855766296387 Acc:0.4183593690395355\n",
            "Train | Step:: 5120 - Loss:2.197927474975586 Acc:0.4193750023841858\n",
            "Train | Step:: 5130 - Loss:2.206932306289673 Acc:0.41874998807907104\n",
            "Train | Step:: 5140 - Loss:2.2148232460021973 Acc:0.41941964626312256\n",
            "Train | Step:: 5150 - Loss:2.220360279083252 Acc:0.41748046875\n",
            "Train | Step:: 5160 - Loss:2.2174038887023926 Acc:0.4185763895511627\n",
            "Train | Step:: 5170 - Loss:2.2053186893463135 Acc:0.42210936546325684\n",
            "Train | Step:: 5180 - Loss:2.196162700653076 Acc:0.4232954680919647\n",
            "Train | Step:: 5190 - Loss:2.194481372833252 Acc:0.4245442748069763\n",
            "Train | Step:: 5200 - Loss:2.1986443996429443 Acc:0.42367789149284363\n",
            "Train | Step:: 5210 - Loss:2.2049129009246826 Acc:0.4224330484867096\n",
            "Train | Step:: 5220 - Loss:2.208613157272339 Acc:0.4215624928474426\n",
            "Train | Step:: 5230 - Loss:2.2148640155792236 Acc:0.4198242127895355\n",
            "Train | Step:: 5240 - Loss:2.217729091644287 Acc:0.4195772111415863\n",
            "Train | Step:: 5250 - Loss:2.2174887657165527 Acc:0.4198784828186035\n",
            "Train | Step:: 5260 - Loss:2.217965602874756 Acc:0.4199013113975525\n",
            "Train | Step:: 5270 - Loss:2.216494560241699 Acc:0.4212109446525574\n",
            "Train | Step:: 5280 - Loss:2.213559865951538 Acc:0.4212425649166107\n",
            "Train | Step:: 5290 - Loss:2.2098536491394043 Acc:0.42201703786849976\n",
            "Train | Step:: 5300 - Loss:2.2028746604919434 Acc:0.423845112323761\n",
            "Train | Step:: 5310 - Loss:2.200575113296509 Acc:0.4246419370174408\n",
            "Train | Step:: 5320 - Loss:2.193587064743042 Acc:0.42615625262260437\n",
            "Train | Step:: 5330 - Loss:2.1908998489379883 Acc:0.4267728328704834\n",
            "Train | Step:: 5340 - Loss:2.187580108642578 Acc:0.4272569417953491\n",
            "Train | Step:: 5350 - Loss:2.18639874458313 Acc:0.4281808137893677\n",
            "Train | Step:: 5360 - Loss:2.183295965194702 Acc:0.42858296632766724\n",
            "Train | Step:: 5370 - Loss:2.1834287643432617 Acc:0.4288281202316284\n",
            "Train | Step:: 5380 - Loss:2.1785969734191895 Acc:0.4297631084918976\n",
            "Train | Step:: 5390 - Loss:2.176846981048584 Acc:0.43061524629592896\n",
            "Train | Step:: 5400 - Loss:2.1753365993499756 Acc:0.4311079680919647\n",
            "Train | Step:: 5410 - Loss:2.1752490997314453 Acc:0.4312729835510254\n",
            "Train | Step:: 5420 - Loss:2.1705408096313477 Acc:0.432232141494751\n",
            "Train | Step:: 5430 - Loss:2.166166305541992 Acc:0.43292099237442017\n",
            "Train | Step:: 5440 - Loss:2.162677049636841 Acc:0.4335726499557495\n",
            "Train | Step:: 5450 - Loss:2.160555839538574 Acc:0.433634877204895\n",
            "Train | Step:: 5460 - Loss:2.1576006412506104 Acc:0.4342147409915924\n",
            "Train | 14 - Loss:2.1576006412506104 Acc:0.4342147409915924\n",
            "Valid | 14 - Loss:3.173264741897583 Acc:0.2621193826198578\n",
            "Time Taken:: 14: 169.48\n",
            "Model Saved at model_ckpt/vit/ckpt-14\n",
            " 14% 14/100 [40:38<4:05:08, 171.03s/it]\n",
            "\n",
            "Start Training 15\n",
            "\n",
            "\n",
            "Train | Step:: 5470 - Loss:2.057663917541504 Acc:0.46562498807907104\n",
            "Train | Step:: 5480 - Loss:2.0697290897369385 Acc:0.45234376192092896\n",
            "Train | Step:: 5490 - Loss:2.066800117492676 Acc:0.45520833134651184\n",
            "Train | Step:: 5500 - Loss:2.0463337898254395 Acc:0.458984375\n",
            "Train | Step:: 5510 - Loss:2.0593130588531494 Acc:0.45218750834465027\n",
            "Train | Step:: 5520 - Loss:2.0682196617126465 Acc:0.45299479365348816\n",
            "Train | Step:: 5530 - Loss:2.0491385459899902 Acc:0.4575892984867096\n",
            "Train | Step:: 5540 - Loss:2.0601723194122314 Acc:0.45439451932907104\n",
            "Train | Step:: 5550 - Loss:2.0624310970306396 Acc:0.45260417461395264\n",
            "Train | Step:: 5560 - Loss:2.0539181232452393 Acc:0.45484375953674316\n",
            "Train | Step:: 5570 - Loss:2.0423216819763184 Acc:0.4582386314868927\n",
            "Train | Step:: 5580 - Loss:2.043130874633789 Acc:0.4583984315395355\n",
            "Train | Step:: 5590 - Loss:2.046339750289917 Acc:0.4575120210647583\n",
            "Train | Step:: 5600 - Loss:2.0544960498809814 Acc:0.45558035373687744\n",
            "Train | Step:: 5610 - Loss:2.0600056648254395 Acc:0.4546354115009308\n",
            "Train | Step:: 5620 - Loss:2.0661203861236572 Acc:0.45356446504592896\n",
            "Train | Step:: 5630 - Loss:2.0598111152648926 Acc:0.4552389681339264\n",
            "Train | Step:: 5640 - Loss:2.0566389560699463 Acc:0.4563367962837219\n",
            "Train | Step:: 5650 - Loss:2.060729742050171 Acc:0.4560032784938812\n",
            "Train | Step:: 5660 - Loss:2.061289072036743 Acc:0.45542967319488525\n",
            "Train | Step:: 5670 - Loss:2.0616188049316406 Acc:0.4554687440395355\n",
            "Train | Step:: 5680 - Loss:2.056610345840454 Acc:0.45585936307907104\n",
            "Train | Step:: 5690 - Loss:2.048466920852661 Acc:0.45822009444236755\n",
            "Train | Step:: 5700 - Loss:2.043945789337158 Acc:0.45966798067092896\n",
            "Train | Step:: 5710 - Loss:2.037071943283081 Acc:0.46171873807907104\n",
            "Train | Step:: 5720 - Loss:2.0348868370056152 Acc:0.4619290828704834\n",
            "Train | Step:: 5730 - Loss:2.0363128185272217 Acc:0.4616898000240326\n",
            "Train | Step:: 5740 - Loss:2.040161609649658 Acc:0.46130022406578064\n",
            "Train | Step:: 5750 - Loss:2.0411033630371094 Acc:0.4613146483898163\n",
            "Train | Step:: 5760 - Loss:2.041734218597412 Acc:0.4615885317325592\n",
            "Train | Step:: 5770 - Loss:2.0373356342315674 Acc:0.4628276228904724\n",
            "Train | Step:: 5780 - Loss:2.0356478691101074 Acc:0.4637451171875\n",
            "Train | Step:: 5790 - Loss:2.0344436168670654 Acc:0.46403881907463074\n",
            "Train | Step:: 5800 - Loss:2.033337116241455 Acc:0.46438419818878174\n",
            "Train | Step:: 5810 - Loss:2.0299057960510254 Acc:0.4649553596973419\n",
            "Train | Step:: 5820 - Loss:2.023632764816284 Acc:0.4659288227558136\n",
            "Train | Step:: 5830 - Loss:2.0187458992004395 Acc:0.46716639399528503\n",
            "Train | Step:: 5840 - Loss:2.012655735015869 Acc:0.4682360291481018\n",
            "Train | Step:: 5850 - Loss:2.00742506980896 Acc:0.4696314036846161\n",
            "Train | 15 - Loss:2.00742506980896 Acc:0.4696314036846161\n",
            "Valid | 15 - Loss:3.1460795402526855 Acc:0.27443909645080566\n",
            "Time Taken:: 15: 169.67\n",
            "Model Saved at model_ckpt/vit/ckpt-15\n",
            " 15% 15/100 [43:28<4:02:05, 170.89s/it]\n",
            "\n",
            "Start Training 16\n",
            "\n",
            "\n",
            "Train | Step:: 5860 - Loss:1.8805139064788818 Acc:0.49296873807907104\n",
            "Train | Step:: 5870 - Loss:1.8880008459091187 Acc:0.49609375\n",
            "Train | Step:: 5880 - Loss:1.8799186944961548 Acc:0.49661457538604736\n",
            "Train | Step:: 5890 - Loss:1.8531593084335327 Acc:0.5023437738418579\n",
            "Train | Step:: 5900 - Loss:1.8483436107635498 Acc:0.503125011920929\n",
            "Train | Step:: 5910 - Loss:1.8644393682479858 Acc:0.501171886920929\n",
            "Train | Step:: 5920 - Loss:1.8567394018173218 Acc:0.5030133724212646\n",
            "Train | Step:: 5930 - Loss:1.8644853830337524 Acc:0.501660168170929\n",
            "Train | Step:: 5940 - Loss:1.8857686519622803 Acc:0.4950520694255829\n",
            "Train | Step:: 5950 - Loss:1.891740083694458 Acc:0.4945312440395355\n",
            "Train | Step:: 5960 - Loss:1.885725498199463 Acc:0.4961647689342499\n",
            "Train | Step:: 5970 - Loss:1.8889738321304321 Acc:0.49563801288604736\n",
            "Train | Step:: 5980 - Loss:1.9006175994873047 Acc:0.4923076927661896\n",
            "Train | Step:: 5990 - Loss:1.9105255603790283 Acc:0.490234375\n",
            "Train | Step:: 6000 - Loss:1.914886236190796 Acc:0.4892708361148834\n",
            "Train | Step:: 6010 - Loss:1.9214494228363037 Acc:0.48784178495407104\n",
            "Train | Step:: 6020 - Loss:1.9170925617218018 Acc:0.48910844326019287\n",
            "Train | Step:: 6030 - Loss:1.9173476696014404 Acc:0.48836806416511536\n",
            "Train | Step:: 6040 - Loss:1.917343020439148 Acc:0.48943257331848145\n",
            "Train | Step:: 6050 - Loss:1.915969967842102 Acc:0.4899218678474426\n",
            "Train | Step:: 6060 - Loss:1.9159008264541626 Acc:0.48991814255714417\n",
            "Train | Step:: 6070 - Loss:1.9135133028030396 Acc:0.49066051840782166\n",
            "Train | Step:: 6080 - Loss:1.908616065979004 Acc:0.49167799949645996\n",
            "Train | Step:: 6090 - Loss:1.9060598611831665 Acc:0.49205729365348816\n",
            "Train | Step:: 6100 - Loss:1.9018833637237549 Acc:0.4934999942779541\n",
            "Train | Step:: 6110 - Loss:1.9014240503311157 Acc:0.49296873807907104\n",
            "Train | Step:: 6120 - Loss:1.9022947549819946 Acc:0.4927951395511627\n",
            "Train | Step:: 6130 - Loss:1.9036318063735962 Acc:0.4928571283817291\n",
            "Train | Step:: 6140 - Loss:1.9040381908416748 Acc:0.4924299716949463\n",
            "Train | Step:: 6150 - Loss:1.9065569639205933 Acc:0.4917447865009308\n",
            "Train | Step:: 6160 - Loss:1.9014638662338257 Acc:0.4927419424057007\n",
            "Train | Step:: 6170 - Loss:1.8998448848724365 Acc:0.4930664002895355\n",
            "Train | Step:: 6180 - Loss:1.8979980945587158 Acc:0.4937736690044403\n",
            "Train | Step:: 6190 - Loss:1.8980751037597656 Acc:0.4937729835510254\n",
            "Train | Step:: 6200 - Loss:1.8943605422973633 Acc:0.4945535659790039\n",
            "Train | Step:: 6210 - Loss:1.8918380737304688 Acc:0.49459636211395264\n",
            "Train | Step:: 6220 - Loss:1.8889919519424438 Acc:0.49533361196517944\n",
            "Train | Step:: 6230 - Loss:1.8832087516784668 Acc:0.4966488480567932\n",
            "Train | Step:: 6240 - Loss:1.878476858139038 Acc:0.49777644872665405\n",
            "Train | 16 - Loss:1.878476858139038 Acc:0.49777644872665405\n",
            "Valid | 16 - Loss:3.1944291591644287 Acc:0.2749398946762085\n",
            "Time Taken:: 16: 169.71\n",
            "Model Saved at model_ckpt/vit/ckpt-16\n",
            " 16% 16/100 [46:19<3:59:07, 170.81s/it]\n",
            "\n",
            "Start Training 17\n",
            "\n",
            "\n",
            "Train | Step:: 6250 - Loss:1.7058274745941162 Acc:0.516406238079071\n",
            "Train | Step:: 6260 - Loss:1.7177988290786743 Acc:0.528515636920929\n",
            "Train | Step:: 6270 - Loss:1.7049548625946045 Acc:0.5335937738418579\n",
            "Train | Step:: 6280 - Loss:1.6654306650161743 Acc:0.543749988079071\n",
            "Train | Step:: 6290 - Loss:1.65134596824646 Acc:0.5487499833106995\n",
            "Train | Step:: 6300 - Loss:1.6649705171585083 Acc:0.5481770634651184\n",
            "Train | Step:: 6310 - Loss:1.6697936058044434 Acc:0.5475446581840515\n",
            "Train | Step:: 6320 - Loss:1.6868311166763306 Acc:0.5428711175918579\n",
            "Train | Step:: 6330 - Loss:1.7032660245895386 Acc:0.5390625\n",
            "Train | Step:: 6340 - Loss:1.7083163261413574 Acc:0.5375000238418579\n",
            "Train | Step:: 6350 - Loss:1.7061338424682617 Acc:0.5384232997894287\n",
            "Train | Step:: 6360 - Loss:1.7120039463043213 Acc:0.5358723998069763\n",
            "Train | Step:: 6370 - Loss:1.7218278646469116 Acc:0.5344350934028625\n",
            "Train | Step:: 6380 - Loss:1.7318620681762695 Acc:0.5317522287368774\n",
            "Train | Step:: 6390 - Loss:1.7455329895019531 Acc:0.5301041603088379\n",
            "Train | Step:: 6400 - Loss:1.7608922719955444 Acc:0.5279785394668579\n",
            "Train | Step:: 6410 - Loss:1.7608327865600586 Acc:0.5274816155433655\n",
            "Train | Step:: 6420 - Loss:1.7654998302459717 Acc:0.5269097089767456\n",
            "Train | Step:: 6430 - Loss:1.7679088115692139 Acc:0.5259868502616882\n",
            "Train | Step:: 6440 - Loss:1.7642594575881958 Acc:0.5274999737739563\n",
            "Train | Step:: 6450 - Loss:1.7638522386550903 Acc:0.5273065567016602\n",
            "Train | Step:: 6460 - Loss:1.7590051889419556 Acc:0.5287997126579285\n",
            "Train | Step:: 6470 - Loss:1.751947283744812 Acc:0.530978262424469\n",
            "Train | Step:: 6480 - Loss:1.7448192834854126 Acc:0.5326172113418579\n",
            "Train | Step:: 6490 - Loss:1.7417690753936768 Acc:0.5330937504768372\n",
            "Train | Step:: 6500 - Loss:1.7393817901611328 Acc:0.5336238145828247\n",
            "Train | Step:: 6510 - Loss:1.7363404035568237 Acc:0.5341145992279053\n",
            "Train | Step:: 6520 - Loss:1.7345067262649536 Acc:0.5347656011581421\n",
            "Train | Step:: 6530 - Loss:1.7313295602798462 Acc:0.5350484848022461\n",
            "Train | Step:: 6540 - Loss:1.729675054550171 Acc:0.5358593463897705\n",
            "Train | Step:: 6550 - Loss:1.722436547279358 Acc:0.5380292534828186\n",
            "Train | Step:: 6560 - Loss:1.7202584743499756 Acc:0.538500964641571\n",
            "Train | Step:: 6570 - Loss:1.7174782752990723 Acc:0.5396543741226196\n",
            "Train | Step:: 6580 - Loss:1.717803716659546 Acc:0.539958655834198\n",
            "Train | Step:: 6590 - Loss:1.7166396379470825 Acc:0.5400669574737549\n",
            "Train | Step:: 6600 - Loss:1.713688611984253 Acc:0.5407769083976746\n",
            "Train | Step:: 6610 - Loss:1.711629867553711 Acc:0.5411528944969177\n",
            "Train | Step:: 6620 - Loss:1.709185004234314 Acc:0.5415090322494507\n",
            "Train | Step:: 6630 - Loss:1.7076185941696167 Acc:0.5419270992279053\n",
            "Train | 17 - Loss:1.7076185941696167 Acc:0.5419270992279053\n",
            "Valid | 17 - Loss:3.29060435295105 Acc:0.26762819290161133\n",
            "Time Taken:: 17: 169.70\n",
            "Model Saved at model_ckpt/vit/ckpt-17\n",
            " 17% 17/100 [49:09<3:56:12, 170.75s/it]\n",
            "\n",
            "Start Training 18\n",
            "\n",
            "\n",
            "Train | Step:: 6640 - Loss:1.6004855632781982 Acc:0.56640625\n",
            "Train | Step:: 6650 - Loss:1.5807502269744873 Acc:0.5667968988418579\n",
            "Train | Step:: 6660 - Loss:1.5747946500778198 Acc:0.5674479007720947\n",
            "Train | Step:: 6670 - Loss:1.5567593574523926 Acc:0.5708984136581421\n",
            "Train | Step:: 6680 - Loss:1.542589545249939 Acc:0.5748437643051147\n",
            "Train | Step:: 6690 - Loss:1.550020456314087 Acc:0.576171875\n",
            "Train | Step:: 6700 - Loss:1.557024598121643 Acc:0.5744419693946838\n",
            "Train | Step:: 6710 - Loss:1.5658782720565796 Acc:0.571582019329071\n",
            "Train | Step:: 6720 - Loss:1.5812947750091553 Acc:0.5685763955116272\n",
            "Train | Step:: 6730 - Loss:1.5804742574691772 Acc:0.5681250095367432\n",
            "Train | Step:: 6740 - Loss:1.5764989852905273 Acc:0.5676846504211426\n",
            "Train | Step:: 6750 - Loss:1.5766726732254028 Acc:0.5673177242279053\n",
            "Train | Step:: 6760 - Loss:1.5809284448623657 Acc:0.5677884817123413\n",
            "Train | Step:: 6770 - Loss:1.5864418745040894 Acc:0.5674107074737549\n",
            "Train | Step:: 6780 - Loss:1.596189022064209 Acc:0.5655208230018616\n",
            "Train | Step:: 6790 - Loss:1.6043437719345093 Acc:0.563916027545929\n",
            "Train | Step:: 6800 - Loss:1.6084476709365845 Acc:0.5634650588035583\n",
            "Train | Step:: 6810 - Loss:1.60855233669281 Acc:0.5640624761581421\n",
            "Train | Step:: 6820 - Loss:1.6091349124908447 Acc:0.5643914341926575\n",
            "Train | Step:: 6830 - Loss:1.6118192672729492 Acc:0.5634765625\n",
            "Train | Step:: 6840 - Loss:1.6087253093719482 Acc:0.5640252828598022\n",
            "Train | Step:: 6850 - Loss:1.606620192527771 Acc:0.564240038394928\n",
            "Train | Step:: 6860 - Loss:1.6018006801605225 Acc:0.565319299697876\n",
            "Train | Step:: 6870 - Loss:1.601235270500183 Acc:0.5657551884651184\n",
            "Train | Step:: 6880 - Loss:1.5984207391738892 Acc:0.5664374828338623\n",
            "Train | Step:: 6890 - Loss:1.594573736190796 Acc:0.567578136920929\n",
            "Train | Step:: 6900 - Loss:1.5883125066757202 Acc:0.5687500238418579\n",
            "Train | Step:: 6910 - Loss:1.5858399868011475 Acc:0.5698102712631226\n",
            "Train | Step:: 6920 - Loss:1.5819096565246582 Acc:0.5707435607910156\n",
            "Train | Step:: 6930 - Loss:1.5798956155776978 Acc:0.5715104341506958\n",
            "Train | Step:: 6940 - Loss:1.573484182357788 Acc:0.5731098651885986\n",
            "Train | Step:: 6950 - Loss:1.568017840385437 Acc:0.574511706829071\n",
            "Train | Step:: 6960 - Loss:1.5640896558761597 Acc:0.5753551125526428\n",
            "Train | Step:: 6970 - Loss:1.5626202821731567 Acc:0.5760110020637512\n",
            "Train | Step:: 6980 - Loss:1.5591347217559814 Acc:0.5768303275108337\n",
            "Train | Step:: 6990 - Loss:1.5575169324874878 Acc:0.5772569179534912\n",
            "Train | Step:: 7000 - Loss:1.5571571588516235 Acc:0.5775760412216187\n",
            "Train | Step:: 7010 - Loss:1.5565218925476074 Acc:0.5778577327728271\n",
            "Train | Step:: 7020 - Loss:1.554956316947937 Acc:0.5780248641967773\n",
            "Train | 18 - Loss:1.554956316947937 Acc:0.5780248641967773\n",
            "Valid | 18 - Loss:3.3320837020874023 Acc:0.2660256326198578\n",
            "Time Taken:: 18: 169.71\n",
            "Model Saved at model_ckpt/vit/ckpt-18\n",
            " 18% 18/100 [52:00<3:53:27, 170.82s/it]\n",
            "\n",
            "Start Training 19\n",
            "\n",
            "\n",
            "Train | Step:: 7030 - Loss:1.5886807441711426 Acc:0.559374988079071\n",
            "Train | Step:: 7040 - Loss:1.537314534187317 Acc:0.572265625\n",
            "Train | Step:: 7050 - Loss:1.5327879190444946 Acc:0.5776041746139526\n",
            "Train | Step:: 7060 - Loss:1.4813077449798584 Acc:0.589648425579071\n",
            "Train | Step:: 7070 - Loss:1.4833565950393677 Acc:0.5907812714576721\n",
            "Train | Step:: 7080 - Loss:1.4677153825759888 Acc:0.5955728888511658\n",
            "Train | Step:: 7090 - Loss:1.464788794517517 Acc:0.5967633724212646\n",
            "Train | Step:: 7100 - Loss:1.465814232826233 Acc:0.5966796875\n",
            "Train | Step:: 7110 - Loss:1.478171944618225 Acc:0.5931423902511597\n",
            "Train | Step:: 7120 - Loss:1.4782743453979492 Acc:0.5928124785423279\n",
            "Train | Step:: 7130 - Loss:1.4768058061599731 Acc:0.592968761920929\n",
            "Train | Step:: 7140 - Loss:1.4770692586898804 Acc:0.5933593511581421\n",
            "Train | Step:: 7150 - Loss:1.483876347541809 Acc:0.5911658406257629\n",
            "Train | Step:: 7160 - Loss:1.489310622215271 Acc:0.5909039974212646\n",
            "Train | Step:: 7170 - Loss:1.4942795038223267 Acc:0.59005206823349\n",
            "Train | Step:: 7180 - Loss:1.5094364881515503 Acc:0.5865722894668579\n",
            "Train | Step:: 7190 - Loss:1.517202615737915 Acc:0.5852022171020508\n",
            "Train | Step:: 7200 - Loss:1.5209152698516846 Acc:0.5849392414093018\n",
            "Train | Step:: 7210 - Loss:1.5162136554718018 Acc:0.585978627204895\n",
            "Train | Step:: 7220 - Loss:1.5142147541046143 Acc:0.5869140625\n",
            "Train | Step:: 7230 - Loss:1.5133426189422607 Acc:0.5872767567634583\n",
            "Train | Step:: 7240 - Loss:1.5122092962265015 Acc:0.5873579382896423\n",
            "Train | Step:: 7250 - Loss:1.5061026811599731 Acc:0.589538037776947\n",
            "Train | Step:: 7260 - Loss:1.501592993736267 Acc:0.5907552242279053\n",
            "Train | Step:: 7270 - Loss:1.4951796531677246 Acc:0.5923125147819519\n",
            "Train | Step:: 7280 - Loss:1.4922846555709839 Acc:0.5935396552085876\n",
            "Train | Step:: 7290 - Loss:1.4863008260726929 Acc:0.5947916507720947\n",
            "Train | Step:: 7300 - Loss:1.4837636947631836 Acc:0.5961495637893677\n",
            "Train | Step:: 7310 - Loss:1.4782390594482422 Acc:0.596875011920929\n",
            "Train | Step:: 7320 - Loss:1.4764130115509033 Acc:0.5977864861488342\n",
            "Train | Step:: 7330 - Loss:1.4705777168273926 Acc:0.5988407135009766\n",
            "Train | Step:: 7340 - Loss:1.4641438722610474 Acc:0.6006103754043579\n",
            "Train | Step:: 7350 - Loss:1.4597562551498413 Acc:0.6016098260879517\n",
            "Train | Step:: 7360 - Loss:1.4586411714553833 Acc:0.6020910143852234\n",
            "Train | Step:: 7370 - Loss:1.4563382863998413 Acc:0.6026339530944824\n",
            "Train | Step:: 7380 - Loss:1.454777717590332 Acc:0.6030598878860474\n",
            "Train | Step:: 7390 - Loss:1.453332543373108 Acc:0.6031249761581421\n",
            "Train | Step:: 7400 - Loss:1.450965166091919 Acc:0.6038240194320679\n",
            "Train | Step:: 7410 - Loss:1.451333999633789 Acc:0.603786051273346\n",
            "Train | 19 - Loss:1.451333999633789 Acc:0.603786051273346\n",
            "Valid | 19 - Loss:3.401996374130249 Acc:0.27223557233810425\n",
            "Time Taken:: 19: 169.71\n",
            "Model Saved at model_ckpt/vit/ckpt-19\n",
            " 19% 19/100 [54:51<3:50:32, 170.77s/it]\n",
            "\n",
            "Start Training 20\n",
            "\n",
            "\n",
            "Train | Step:: 7420 - Loss:1.489227056503296 Acc:0.586718738079071\n",
            "Train | Step:: 7430 - Loss:1.415946364402771 Acc:0.6070312261581421\n",
            "Train | Step:: 7440 - Loss:1.4035720825195312 Acc:0.6135416626930237\n",
            "Train | Step:: 7450 - Loss:1.3895334005355835 Acc:0.6142578125\n",
            "Train | Step:: 7460 - Loss:1.3786883354187012 Acc:0.6173437237739563\n",
            "Train | Step:: 7470 - Loss:1.368979811668396 Acc:0.6200520992279053\n",
            "Train | Step:: 7480 - Loss:1.3560495376586914 Acc:0.624218761920929\n",
            "Train | Step:: 7490 - Loss:1.3508237600326538 Acc:0.626171886920929\n",
            "Train | Step:: 7500 - Loss:1.368087649345398 Acc:0.6214409470558167\n",
            "Train | Step:: 7510 - Loss:1.3770074844360352 Acc:0.6193749904632568\n",
            "Train | Step:: 7520 - Loss:1.3807668685913086 Acc:0.6183949112892151\n",
            "Train | Step:: 7530 - Loss:1.381999135017395 Acc:0.6181640625\n",
            "Train | Step:: 7540 - Loss:1.3946958780288696 Acc:0.6149038672447205\n",
            "Train | Step:: 7550 - Loss:1.3988984823226929 Acc:0.6148437261581421\n",
            "Train | Step:: 7560 - Loss:1.3967291116714478 Acc:0.6163020730018616\n",
            "Train | Step:: 7570 - Loss:1.4042503833770752 Acc:0.6142578125\n",
            "Train | Step:: 7580 - Loss:1.4018523693084717 Acc:0.614613950252533\n",
            "Train | Step:: 7590 - Loss:1.4018638134002686 Acc:0.6146267652511597\n",
            "Train | Step:: 7600 - Loss:1.3981809616088867 Acc:0.6160361766815186\n",
            "Train | Step:: 7610 - Loss:1.3952805995941162 Acc:0.6172265410423279\n",
            "Train | Step:: 7620 - Loss:1.3941748142242432 Acc:0.6175967454910278\n",
            "Train | Step:: 7630 - Loss:1.3922812938690186 Acc:0.6176136136054993\n",
            "Train | Step:: 7640 - Loss:1.387639045715332 Acc:0.6195991635322571\n",
            "Train | Step:: 7650 - Loss:1.3836560249328613 Acc:0.6205078363418579\n",
            "Train | Step:: 7660 - Loss:1.3809353113174438 Acc:0.620968759059906\n",
            "Train | Step:: 7670 - Loss:1.379069209098816 Acc:0.6215444803237915\n",
            "Train | Step:: 7680 - Loss:1.3761012554168701 Acc:0.6217013597488403\n",
            "Train | Step:: 7690 - Loss:1.3728841543197632 Acc:0.6225167512893677\n",
            "Train | Step:: 7700 - Loss:1.3669240474700928 Acc:0.624110996723175\n",
            "Train | Step:: 7710 - Loss:1.3614014387130737 Acc:0.6259375214576721\n",
            "Train | Step:: 7720 - Loss:1.356338620185852 Acc:0.6267893314361572\n",
            "Train | Step:: 7730 - Loss:1.3511120080947876 Acc:0.6280273199081421\n",
            "Train | Step:: 7740 - Loss:1.3451756238937378 Acc:0.6296638250350952\n",
            "Train | Step:: 7750 - Loss:1.340538501739502 Acc:0.631135106086731\n",
            "Train | Step:: 7760 - Loss:1.3362584114074707 Acc:0.6322767734527588\n",
            "Train | Step:: 7770 - Loss:1.3337453603744507 Acc:0.632617175579071\n",
            "Train | Step:: 7780 - Loss:1.333943486213684 Acc:0.6325802206993103\n",
            "Train | Step:: 7790 - Loss:1.3352844715118408 Acc:0.6319490075111389\n",
            "Train | Step:: 7800 - Loss:1.3338016271591187 Acc:0.6321914792060852\n",
            "Train | 20 - Loss:1.3338016271591187 Acc:0.6321914792060852\n",
            "Valid | 20 - Loss:3.5899267196655273 Acc:0.26362180709838867\n",
            "Time Taken:: 20: 170.02\n",
            "Model Saved at model_ckpt/vit/ckpt-20\n",
            " 20% 20/100 [57:42<3:47:47, 170.84s/it]\n",
            "\n",
            "Start Training 21\n",
            "\n",
            "\n",
            "Train | Step:: 7810 - Loss:1.274762511253357 Acc:0.660937488079071\n",
            "Train | Step:: 7820 - Loss:1.2314159870147705 Acc:0.669921875\n",
            "Train | Step:: 7830 - Loss:1.213752269744873 Acc:0.6697916388511658\n",
            "Train | Step:: 7840 - Loss:1.2121922969818115 Acc:0.668164074420929\n",
            "Train | Step:: 7850 - Loss:1.2042750120162964 Acc:0.66796875\n",
            "Train | Step:: 7860 - Loss:1.2139145135879517 Acc:0.6682291626930237\n",
            "Train | Step:: 7870 - Loss:1.2068992853164673 Acc:0.6686384081840515\n",
            "Train | Step:: 7880 - Loss:1.207061529159546 Acc:0.6673828363418579\n",
            "Train | Step:: 7890 - Loss:1.2242555618286133 Acc:0.663281261920929\n",
            "Train | Step:: 7900 - Loss:1.2350982427597046 Acc:0.6596875190734863\n",
            "Train | Step:: 7910 - Loss:1.2348073720932007 Acc:0.6596590876579285\n",
            "Train | Step:: 7920 - Loss:1.2371821403503418 Acc:0.6585286259651184\n",
            "Train | Step:: 7930 - Loss:1.2467002868652344 Acc:0.6568509340286255\n",
            "Train | Step:: 7940 - Loss:1.2549117803573608 Acc:0.6551339030265808\n",
            "Train | Step:: 7950 - Loss:1.2562962770462036 Acc:0.655260443687439\n",
            "Train | Step:: 7960 - Loss:1.2555021047592163 Acc:0.655957043170929\n",
            "Train | Step:: 7970 - Loss:1.2493959665298462 Acc:0.6582261323928833\n",
            "Train | Step:: 7980 - Loss:1.244395136833191 Acc:0.6598524451255798\n",
            "Train | Step:: 7990 - Loss:1.2383701801300049 Acc:0.6613075733184814\n",
            "Train | Step:: 8000 - Loss:1.2344825267791748 Acc:0.6619921922683716\n",
            "Train | Step:: 8010 - Loss:1.2287192344665527 Acc:0.664248526096344\n",
            "Train | Step:: 8020 - Loss:1.2262150049209595 Acc:0.6643821001052856\n",
            "Train | Step:: 8030 - Loss:1.220466136932373 Acc:0.666066586971283\n",
            "Train | Step:: 8040 - Loss:1.2158278226852417 Acc:0.6678711175918579\n",
            "Train | Step:: 8050 - Loss:1.2084336280822754 Acc:0.6695312261581421\n",
            "Train | Step:: 8060 - Loss:1.205635905265808 Acc:0.6701021790504456\n",
            "Train | Step:: 8070 - Loss:1.2019436359405518 Acc:0.6706308126449585\n",
            "Train | Step:: 8080 - Loss:1.1967240571975708 Acc:0.672656238079071\n",
            "Train | Step:: 8090 - Loss:1.189428448677063 Acc:0.6740301847457886\n",
            "Train | Step:: 8100 - Loss:1.1865726709365845 Acc:0.6742708086967468\n",
            "Train | Step:: 8110 - Loss:1.1789977550506592 Acc:0.6763356924057007\n",
            "Train | Step:: 8120 - Loss:1.174439787864685 Acc:0.6774657964706421\n",
            "Train | Step:: 8130 - Loss:1.1703214645385742 Acc:0.6791430115699768\n",
            "Train | Step:: 8140 - Loss:1.163456678390503 Acc:0.6809282898902893\n",
            "Train | Step:: 8150 - Loss:1.157407522201538 Acc:0.6827232241630554\n",
            "Train | Step:: 8160 - Loss:1.1533900499343872 Acc:0.6839627027511597\n",
            "Train | Step:: 8170 - Loss:1.1496291160583496 Acc:0.6847761869430542\n",
            "Train | Step:: 8180 - Loss:1.1505849361419678 Acc:0.684374988079071\n",
            "Train | Step:: 8190 - Loss:1.1507874727249146 Acc:0.6843549609184265\n",
            "Train | 21 - Loss:1.1507874727249146 Acc:0.6843549609184265\n",
            "Valid | 21 - Loss:3.67328143119812 Acc:0.25771233439445496\n",
            "Time Taken:: 21: 169.97\n",
            "Model Saved at model_ckpt/vit/ckpt-21\n",
            " 21% 21/100 [1:00:33<3:44:56, 170.84s/it]\n",
            "\n",
            "Start Training 22\n",
            "\n",
            "\n",
            "Train | Step:: 8200 - Loss:1.083416223526001 Acc:0.703906238079071\n",
            "Train | Step:: 8210 - Loss:1.053829312324524 Acc:0.712890625\n",
            "Train | Step:: 8220 - Loss:1.0371155738830566 Acc:0.7169271111488342\n",
            "Train | Step:: 8230 - Loss:1.027388334274292 Acc:0.7144531011581421\n",
            "Train | Step:: 8240 - Loss:1.0394423007965088 Acc:0.7065625190734863\n",
            "Train | Step:: 8250 - Loss:1.0413310527801514 Acc:0.7072916626930237\n",
            "Train | Step:: 8260 - Loss:1.0560210943222046 Acc:0.7043526768684387\n",
            "Train | Step:: 8270 - Loss:1.0476584434509277 Acc:0.7061523199081421\n",
            "Train | Step:: 8280 - Loss:1.0502581596374512 Acc:0.7059895992279053\n",
            "Train | Step:: 8290 - Loss:1.0614796876907349 Acc:0.7045312523841858\n",
            "Train | Step:: 8300 - Loss:1.059989333152771 Acc:0.7051846385002136\n",
            "Train | Step:: 8310 - Loss:1.0616917610168457 Acc:0.7046874761581421\n",
            "Train | Step:: 8320 - Loss:1.0682034492492676 Acc:0.7036658525466919\n",
            "Train | Step:: 8330 - Loss:1.0727570056915283 Acc:0.7025669813156128\n",
            "Train | Step:: 8340 - Loss:1.0746039152145386 Acc:0.702343761920929\n",
            "Train | Step:: 8350 - Loss:1.0745680332183838 Acc:0.702197253704071\n",
            "Train | Step:: 8360 - Loss:1.0709387063980103 Acc:0.7029411792755127\n",
            "Train | Step:: 8370 - Loss:1.0701189041137695 Acc:0.7032986283302307\n",
            "Train | Step:: 8380 - Loss:1.0697591304779053 Acc:0.7029605507850647\n",
            "Train | Step:: 8390 - Loss:1.0665520429611206 Acc:0.7035937309265137\n",
            "Train | Step:: 8400 - Loss:1.063262939453125 Acc:0.7043526768684387\n",
            "Train | Step:: 8410 - Loss:1.058375597000122 Acc:0.7059304118156433\n",
            "Train | Step:: 8420 - Loss:1.0540663003921509 Acc:0.707065224647522\n",
            "Train | Step:: 8430 - Loss:1.0477224588394165 Acc:0.7086263298988342\n",
            "Train | Step:: 8440 - Loss:1.0431100130081177 Acc:0.7099375128746033\n",
            "Train | Step:: 8450 - Loss:1.0423935651779175 Acc:0.7103365659713745\n",
            "Train | Step:: 8460 - Loss:1.0361688137054443 Acc:0.711718738079071\n",
            "Train | Step:: 8470 - Loss:1.0342094898223877 Acc:0.7128069400787354\n",
            "Train | Step:: 8480 - Loss:1.0287717580795288 Acc:0.7139816880226135\n",
            "Train | Step:: 8490 - Loss:1.025522232055664 Acc:0.71484375\n",
            "Train | Step:: 8500 - Loss:1.0201051235198975 Acc:0.7167338728904724\n",
            "Train | Step:: 8510 - Loss:1.016847014427185 Acc:0.7180420160293579\n",
            "Train | Step:: 8520 - Loss:1.0145509243011475 Acc:0.719081461429596\n",
            "Train | Step:: 8530 - Loss:1.0097662210464478 Acc:0.7205193042755127\n",
            "Train | Step:: 8540 - Loss:1.0041778087615967 Acc:0.7216517925262451\n",
            "Train | Step:: 8550 - Loss:0.9991747736930847 Acc:0.722851574420929\n",
            "Train | Step:: 8560 - Loss:0.9947233200073242 Acc:0.7243031859397888\n",
            "Train | Step:: 8570 - Loss:0.9910483360290527 Acc:0.7253289222717285\n",
            "Train | Step:: 8580 - Loss:0.9925269484519958 Acc:0.7249799966812134\n",
            "Train | 22 - Loss:0.9925269484519958 Acc:0.7249799966812134\n",
            "Valid | 22 - Loss:3.9207074642181396 Acc:0.2649238705635071\n",
            "Time Taken:: 22: 169.67\n",
            "Model Saved at model_ckpt/vit/ckpt-22\n",
            " 22% 22/100 [1:03:24<3:41:59, 170.76s/it]\n",
            "\n",
            "Start Training 23\n",
            "\n",
            "\n",
            "Train | Step:: 8590 - Loss:1.1573781967163086 Acc:0.6820312738418579\n",
            "Train | Step:: 8600 - Loss:1.0679280757904053 Acc:0.6996093988418579\n",
            "Train | Step:: 8610 - Loss:1.0269862413406372 Acc:0.7106770873069763\n",
            "Train | Step:: 8620 - Loss:0.9699212312698364 Acc:0.727734386920929\n",
            "Train | Step:: 8630 - Loss:0.9614325761795044 Acc:0.7284374833106995\n",
            "Train | Step:: 8640 - Loss:0.9549276828765869 Acc:0.7290364503860474\n",
            "Train | Step:: 8650 - Loss:0.9508271217346191 Acc:0.7316964268684387\n",
            "Train | Step:: 8660 - Loss:0.94013911485672 Acc:0.7347656488418579\n",
            "Train | Step:: 8670 - Loss:0.9385111331939697 Acc:0.7353298664093018\n",
            "Train | Step:: 8680 - Loss:0.9420106410980225 Acc:0.733593761920929\n",
            "Train | Step:: 8690 - Loss:0.9465879201889038 Acc:0.7314630746841431\n",
            "Train | Step:: 8700 - Loss:0.9373778104782104 Acc:0.734570324420929\n",
            "Train | Step:: 8710 - Loss:0.9318229556083679 Acc:0.7368389368057251\n",
            "Train | Step:: 8720 - Loss:0.9286407232284546 Acc:0.7380580306053162\n",
            "Train | Step:: 8730 - Loss:0.9312018752098083 Acc:0.7382291555404663\n",
            "Train | Step:: 8740 - Loss:0.9315539598464966 Acc:0.737988293170929\n",
            "Train | Step:: 8750 - Loss:0.9276683330535889 Acc:0.7392922639846802\n",
            "Train | Step:: 8760 - Loss:0.9260473251342773 Acc:0.7404080033302307\n",
            "Train | Step:: 8770 - Loss:0.9209898114204407 Acc:0.74210524559021\n",
            "Train | Step:: 8780 - Loss:0.9156239032745361 Acc:0.7440234422683716\n",
            "Train | Step:: 8790 - Loss:0.9129469394683838 Acc:0.745126485824585\n",
            "Train | Step:: 8800 - Loss:0.9105299115180969 Acc:0.74609375\n",
            "Train | Step:: 8810 - Loss:0.9046962857246399 Acc:0.747520387172699\n",
            "Train | Step:: 8820 - Loss:0.897367537021637 Acc:0.749804675579071\n",
            "Train | Step:: 8830 - Loss:0.8905829191207886 Acc:0.7516250014305115\n",
            "Train | Step:: 8840 - Loss:0.8828451633453369 Acc:0.7537860870361328\n",
            "Train | Step:: 8850 - Loss:0.8768982887268066 Acc:0.7552661895751953\n",
            "Train | Step:: 8860 - Loss:0.8744935989379883 Acc:0.7558872699737549\n",
            "Train | Step:: 8870 - Loss:0.8684040904045105 Acc:0.7574083805084229\n",
            "Train | Step:: 8880 - Loss:0.8646107912063599 Acc:0.7586197853088379\n",
            "Train | Step:: 8890 - Loss:0.8614789247512817 Acc:0.7592993974685669\n",
            "Train | Step:: 8900 - Loss:0.860700786113739 Acc:0.759838879108429\n",
            "Train | Step:: 8910 - Loss:0.8606142997741699 Acc:0.7598484754562378\n",
            "Train | Step:: 8920 - Loss:0.8579522371292114 Acc:0.760661780834198\n",
            "Train | Step:: 8930 - Loss:0.8541070818901062 Acc:0.7618526816368103\n",
            "Train | Step:: 8940 - Loss:0.848815381526947 Acc:0.7632812261581421\n",
            "Train | Step:: 8950 - Loss:0.8463175892829895 Acc:0.7638302445411682\n",
            "Train | Step:: 8960 - Loss:0.8428323268890381 Acc:0.7648026347160339\n",
            "Train | Step:: 8970 - Loss:0.8413815498352051 Acc:0.7652043104171753\n",
            "Train | 23 - Loss:0.8413815498352051 Acc:0.7652043104171753\n",
            "Valid | 23 - Loss:3.9636096954345703 Acc:0.26372194290161133\n",
            "Time Taken:: 23: 169.89\n",
            "Model Saved at model_ckpt/vit/ckpt-23\n",
            " 23% 23/100 [1:06:14<3:39:09, 170.77s/it]\n",
            "\n",
            "Start Training 24\n",
            "\n",
            "\n",
            "Train | Step:: 8980 - Loss:0.9512706995010376 Acc:0.734375\n",
            "Train | Step:: 8990 - Loss:0.9092825651168823 Acc:0.7406250238418579\n",
            "Train | Step:: 9000 - Loss:0.8750482797622681 Acc:0.7526041865348816\n",
            "Train | Step:: 9010 - Loss:0.8518649339675903 Acc:0.7591797113418579\n",
            "Train | Step:: 9020 - Loss:0.8288397192955017 Acc:0.766406238079071\n",
            "Train | Step:: 9030 - Loss:0.8200982809066772 Acc:0.770703136920929\n",
            "Train | Step:: 9040 - Loss:0.8115463852882385 Acc:0.7710937261581421\n",
            "Train | Step:: 9050 - Loss:0.8056371808052063 Acc:0.77197265625\n",
            "Train | Step:: 9060 - Loss:0.8003911972045898 Acc:0.7729166746139526\n",
            "Train | Step:: 9070 - Loss:0.7976589798927307 Acc:0.7733593583106995\n",
            "Train | Step:: 9080 - Loss:0.7922147512435913 Acc:0.7748579382896423\n",
            "Train | Step:: 9090 - Loss:0.7882974743843079 Acc:0.7761067748069763\n",
            "Train | Step:: 9100 - Loss:0.7840974926948547 Acc:0.7787860631942749\n",
            "Train | Step:: 9110 - Loss:0.7858275771141052 Acc:0.7780134081840515\n",
            "Train | Step:: 9120 - Loss:0.7853823900222778 Acc:0.7789062261581421\n",
            "Train | Step:: 9130 - Loss:0.7885034680366516 Acc:0.7791503667831421\n",
            "Train | Step:: 9140 - Loss:0.7889105677604675 Acc:0.7793198823928833\n",
            "Train | Step:: 9150 - Loss:0.7929831743240356 Acc:0.7787760496139526\n",
            "Train | Step:: 9160 - Loss:0.7932203412055969 Acc:0.7794407606124878\n",
            "Train | Step:: 9170 - Loss:0.7867559790611267 Acc:0.7816015481948853\n",
            "Train | Step:: 9180 - Loss:0.7815933227539062 Acc:0.7834077477455139\n",
            "Train | Step:: 9190 - Loss:0.7775417566299438 Acc:0.7848366498947144\n",
            "Train | Step:: 9200 - Loss:0.7712302803993225 Acc:0.786888599395752\n",
            "Train | Step:: 9210 - Loss:0.7666777968406677 Acc:0.7884765863418579\n",
            "Train | Step:: 9220 - Loss:0.7602692246437073 Acc:0.7901874780654907\n",
            "Train | Step:: 9230 - Loss:0.7560219168663025 Acc:0.7912560105323792\n",
            "Train | Step:: 9240 - Loss:0.7508531212806702 Acc:0.7920138835906982\n",
            "Train | Step:: 9250 - Loss:0.748042643070221 Acc:0.7930524349212646\n",
            "Train | Step:: 9260 - Loss:0.7442068457603455 Acc:0.7939655184745789\n",
            "Train | Step:: 9270 - Loss:0.7423381209373474 Acc:0.7947396039962769\n",
            "Train | Step:: 9280 - Loss:0.7392391562461853 Acc:0.7957661151885986\n",
            "Train | Step:: 9290 - Loss:0.7388602495193481 Acc:0.795825183391571\n",
            "Train | Step:: 9300 - Loss:0.7354838252067566 Acc:0.7968276739120483\n",
            "Train | Step:: 9310 - Loss:0.7346274256706238 Acc:0.796989917755127\n",
            "Train | Step:: 9320 - Loss:0.7312099933624268 Acc:0.7977455258369446\n",
            "Train | Step:: 9330 - Loss:0.7279295921325684 Acc:0.7986544966697693\n",
            "Train | Step:: 9340 - Loss:0.7241805791854858 Acc:0.7999155521392822\n",
            "Train | Step:: 9350 - Loss:0.7208482623100281 Acc:0.8008429408073425\n",
            "Train | Step:: 9360 - Loss:0.7189679145812988 Acc:0.8014423251152039\n",
            "Train | 24 - Loss:0.7189679145812988 Acc:0.8014423251152039\n",
            "Valid | 24 - Loss:3.9920966625213623 Acc:0.2699318826198578\n",
            "Time Taken:: 24: 169.86\n",
            "Model Saved at model_ckpt/vit/ckpt-24\n",
            " 24% 24/100 [1:09:05<3:36:18, 170.76s/it]\n",
            "\n",
            "Start Training 25\n",
            "\n",
            "\n",
            "Train | Step:: 9370 - Loss:0.9135165214538574 Acc:0.7484375238418579\n",
            "Train | Step:: 9380 - Loss:0.8686686754226685 Acc:0.754687488079071\n",
            "Train | Step:: 9390 - Loss:0.8190991282463074 Acc:0.7744791507720947\n",
            "Train | Step:: 9400 - Loss:0.7919341921806335 Acc:0.7787109613418579\n",
            "Train | Step:: 9410 - Loss:0.7481368184089661 Acc:0.7900000214576721\n",
            "Train | Step:: 9420 - Loss:0.7365351319313049 Acc:0.7940104007720947\n",
            "Train | Step:: 9430 - Loss:0.7260779738426208 Acc:0.7967634201049805\n",
            "Train | Step:: 9440 - Loss:0.7262023687362671 Acc:0.796679675579071\n",
            "Train | Step:: 9450 - Loss:0.707929253578186 Acc:0.8013020753860474\n",
            "Train | Step:: 9460 - Loss:0.696804404258728 Acc:0.8053905963897705\n",
            "Train | Step:: 9470 - Loss:0.6959443092346191 Acc:0.8060369491577148\n",
            "Train | Step:: 9480 - Loss:0.6912047863006592 Acc:0.8073567748069763\n",
            "Train | Step:: 9490 - Loss:0.6884472370147705 Acc:0.8087139129638672\n",
            "Train | Step:: 9500 - Loss:0.6888847947120667 Acc:0.8093191981315613\n",
            "Train | Step:: 9510 - Loss:0.6901582479476929 Acc:0.8091145753860474\n",
            "Train | Step:: 9520 - Loss:0.6926389932632446 Acc:0.80859375\n",
            "Train | Step:: 9530 - Loss:0.6925570964813232 Acc:0.8078584671020508\n",
            "Train | Step:: 9540 - Loss:0.6952672600746155 Acc:0.8075520992279053\n",
            "Train | Step:: 9550 - Loss:0.6959060430526733 Acc:0.8071545958518982\n",
            "Train | Step:: 9560 - Loss:0.6920351982116699 Acc:0.8082031011581421\n",
            "Train | Step:: 9570 - Loss:0.6866539120674133 Acc:0.8104910850524902\n",
            "Train | Step:: 9580 - Loss:0.6844562292098999 Acc:0.8112215995788574\n",
            "Train | Step:: 9590 - Loss:0.6768579483032227 Acc:0.8134171366691589\n",
            "Train | Step:: 9600 - Loss:0.6718284487724304 Acc:0.8147135376930237\n",
            "Train | Step:: 9610 - Loss:0.667394757270813 Acc:0.816031277179718\n",
            "Train | Step:: 9620 - Loss:0.6637775897979736 Acc:0.8170973658561707\n",
            "Train | Step:: 9630 - Loss:0.6606059670448303 Acc:0.8181134462356567\n",
            "Train | Step:: 9640 - Loss:0.6579272747039795 Acc:0.8189452886581421\n",
            "Train | Step:: 9650 - Loss:0.6535236239433289 Acc:0.8205819129943848\n",
            "Train | Step:: 9660 - Loss:0.6526199579238892 Acc:0.8207031488418579\n",
            "Train | Step:: 9670 - Loss:0.651391327381134 Acc:0.8209425210952759\n",
            "Train | Step:: 9680 - Loss:0.6494429111480713 Acc:0.8216797113418579\n",
            "Train | Step:: 9690 - Loss:0.6485951542854309 Acc:0.8220170736312866\n",
            "Train | Step:: 9700 - Loss:0.6476428508758545 Acc:0.8220358490943909\n",
            "Train | Step:: 9710 - Loss:0.644591748714447 Acc:0.8228124976158142\n",
            "Train | Step:: 9720 - Loss:0.6423928737640381 Acc:0.8233724236488342\n",
            "Train | Step:: 9730 - Loss:0.6401315927505493 Acc:0.8241553902626038\n",
            "Train | Step:: 9740 - Loss:0.6362988352775574 Acc:0.8254933953285217\n",
            "Train | Step:: 9750 - Loss:0.6348218321800232 Acc:0.825901448726654\n",
            "Train | 25 - Loss:0.6348218321800232 Acc:0.825901448726654\n",
            "Valid | 25 - Loss:3.9847235679626465 Acc:0.27373796701431274\n",
            "Time Taken:: 25: 169.56\n",
            "Model Saved at model_ckpt/vit/ckpt-25\n",
            " 25% 25/100 [1:11:56<3:33:20, 170.68s/it]\n",
            "\n",
            "Start Training 26\n",
            "\n",
            "\n",
            "Train | Step:: 9760 - Loss:0.7136550545692444 Acc:0.815625011920929\n",
            "Train | Step:: 9770 - Loss:0.699309229850769 Acc:0.814453125\n",
            "Train | Step:: 9780 - Loss:0.7059400081634521 Acc:0.8096354007720947\n",
            "Train | Step:: 9790 - Loss:0.7001699209213257 Acc:0.805859386920929\n",
            "Train | Step:: 9800 - Loss:0.6706283092498779 Acc:0.8135937452316284\n",
            "Train | Step:: 9810 - Loss:0.6698818206787109 Acc:0.81640625\n",
            "Train | Step:: 9820 - Loss:0.6669718027114868 Acc:0.8179687261581421\n",
            "Train | Step:: 9830 - Loss:0.6579357385635376 Acc:0.8204101324081421\n",
            "Train | Step:: 9840 - Loss:0.6392852663993835 Acc:0.8245659470558167\n",
            "Train | Step:: 9850 - Loss:0.6251361966133118 Acc:0.828125\n",
            "Train | Step:: 9860 - Loss:0.6157653331756592 Acc:0.8308238387107849\n",
            "Train | Step:: 9870 - Loss:0.6140504479408264 Acc:0.8303385376930237\n",
            "Train | Step:: 9880 - Loss:0.6121078729629517 Acc:0.8311899304389954\n",
            "Train | Step:: 9890 - Loss:0.6137304902076721 Acc:0.8306919932365417\n",
            "Train | Step:: 9900 - Loss:0.6177123785018921 Acc:0.8296874761581421\n",
            "Train | Step:: 9910 - Loss:0.6184984445571899 Acc:0.828857421875\n",
            "Train | Step:: 9920 - Loss:0.6153507828712463 Acc:0.8292738795280457\n",
            "Train | Step:: 9930 - Loss:0.621017336845398 Acc:0.8276475667953491\n",
            "Train | Step:: 9940 - Loss:0.6246567368507385 Acc:0.8271792531013489\n",
            "Train | Step:: 9950 - Loss:0.6209287047386169 Acc:0.8280078172683716\n",
            "Train | Step:: 9960 - Loss:0.6200919151306152 Acc:0.8285342454910278\n",
            "Train | Step:: 9970 - Loss:0.6152322292327881 Acc:0.8300070762634277\n",
            "Train | Step:: 9980 - Loss:0.6098855137825012 Acc:0.831487774848938\n",
            "Train | Step:: 9990 - Loss:0.6045869588851929 Acc:0.8334635496139526\n",
            "Train | Step:: 10000 - Loss:0.5991163849830627 Acc:0.835281252861023\n",
            "Train | Step:: 10010 - Loss:0.5980957746505737 Acc:0.8360576629638672\n",
            "Train | Step:: 10020 - Loss:0.5944759249687195 Acc:0.8368055820465088\n",
            "Train | Step:: 10030 - Loss:0.5922940373420715 Acc:0.8377510905265808\n",
            "Train | Step:: 10040 - Loss:0.5878446698188782 Acc:0.8388200402259827\n",
            "Train | Step:: 10050 - Loss:0.5849243998527527 Acc:0.8393229246139526\n",
            "Train | Step:: 10060 - Loss:0.5808280110359192 Acc:0.8403729796409607\n",
            "Train | Step:: 10070 - Loss:0.5793611407279968 Acc:0.841113269329071\n",
            "Train | Step:: 10080 - Loss:0.5789820551872253 Acc:0.8408854007720947\n",
            "Train | Step:: 10090 - Loss:0.5777357220649719 Acc:0.8411535024642944\n",
            "Train | Step:: 10100 - Loss:0.5758464336395264 Acc:0.8417187333106995\n",
            "Train | Step:: 10110 - Loss:0.5740107893943787 Acc:0.8423177003860474\n",
            "Train | Step:: 10120 - Loss:0.5721837878227234 Acc:0.8427998423576355\n",
            "Train | Step:: 10130 - Loss:0.5684242248535156 Acc:0.84383225440979\n",
            "Train | Step:: 10140 - Loss:0.5667107105255127 Acc:0.844491183757782\n",
            "Train | 26 - Loss:0.5667107105255127 Acc:0.844491183757782\n",
            "Valid | 26 - Loss:4.001286029815674 Acc:0.2873597741127014\n",
            "Time Taken:: 26: 169.74\n",
            "Model Saved at model_ckpt/vit/ckpt-26\n",
            " 26% 26/100 [1:14:46<3:30:29, 170.67s/it]\n",
            "\n",
            "Start Training 27\n",
            "\n",
            "\n",
            "Train | Step:: 10150 - Loss:0.5655869841575623 Acc:0.8515625\n",
            "Train | Step:: 10160 - Loss:0.5479959845542908 Acc:0.8519531488418579\n",
            "Train | Step:: 10170 - Loss:0.5721725225448608 Acc:0.8424479365348816\n",
            "Train | Step:: 10180 - Loss:0.5604249238967896 Acc:0.8462890386581421\n",
            "Train | Step:: 10190 - Loss:0.5551000833511353 Acc:0.8482812643051147\n",
            "Train | Step:: 10200 - Loss:0.5582515597343445 Acc:0.8477864861488342\n",
            "Train | Step:: 10210 - Loss:0.5663945078849792 Acc:0.8458705544471741\n",
            "Train | Step:: 10220 - Loss:0.5603758096694946 Acc:0.845996081829071\n",
            "Train | Step:: 10230 - Loss:0.5525487661361694 Acc:0.8482638597488403\n",
            "Train | Step:: 10240 - Loss:0.5426592826843262 Acc:0.8509374856948853\n",
            "Train | Step:: 10250 - Loss:0.5381115674972534 Acc:0.8526278138160706\n",
            "Train | Step:: 10260 - Loss:0.5348163843154907 Acc:0.8538411259651184\n",
            "Train | Step:: 10270 - Loss:0.5348061323165894 Acc:0.8539062738418579\n",
            "Train | Step:: 10280 - Loss:0.5339866876602173 Acc:0.8544642925262451\n",
            "Train | Step:: 10290 - Loss:0.5380010008811951 Acc:0.8533333539962769\n",
            "Train | Step:: 10300 - Loss:0.5400814414024353 Acc:0.8531738519668579\n",
            "Train | Step:: 10310 - Loss:0.53948974609375 Acc:0.8531249761581421\n",
            "Train | Step:: 10320 - Loss:0.542722761631012 Acc:0.8522135615348816\n",
            "Train | Step:: 10330 - Loss:0.5460256338119507 Acc:0.8513980507850647\n",
            "Train | Step:: 10340 - Loss:0.5449972152709961 Acc:0.8515625\n",
            "Train | Step:: 10350 - Loss:0.5440369844436646 Acc:0.8520089387893677\n",
            "Train | Step:: 10360 - Loss:0.5437086224555969 Acc:0.8521307110786438\n",
            "Train | Step:: 10370 - Loss:0.5414423942565918 Acc:0.852683424949646\n",
            "Train | Step:: 10380 - Loss:0.5364667773246765 Acc:0.854199230670929\n",
            "Train | Step:: 10390 - Loss:0.5310538411140442 Acc:0.8556562662124634\n",
            "Train | Step:: 10400 - Loss:0.5273672342300415 Acc:0.8566105961799622\n",
            "Train | Step:: 10410 - Loss:0.5266776084899902 Acc:0.8568286895751953\n",
            "Train | Step:: 10420 - Loss:0.5250545740127563 Acc:0.8575614094734192\n",
            "Train | Step:: 10430 - Loss:0.5222288966178894 Acc:0.8580549359321594\n",
            "Train | Step:: 10440 - Loss:0.5194087028503418 Acc:0.8592708110809326\n",
            "Train | Step:: 10450 - Loss:0.5160347819328308 Acc:0.860156238079071\n",
            "Train | Step:: 10460 - Loss:0.5152210593223572 Acc:0.860278308391571\n",
            "Train | Step:: 10470 - Loss:0.5146980881690979 Acc:0.860700786113739\n",
            "Train | Step:: 10480 - Loss:0.5136009454727173 Acc:0.8609604835510254\n",
            "Train | Step:: 10490 - Loss:0.5129383206367493 Acc:0.8609374761581421\n",
            "Train | Step:: 10500 - Loss:0.5107720494270325 Acc:0.8615451455116272\n",
            "Train | Step:: 10510 - Loss:0.5102744102478027 Acc:0.8620143532752991\n",
            "Train | Step:: 10520 - Loss:0.5083686709403992 Acc:0.8626644611358643\n",
            "Train | Step:: 10530 - Loss:0.5066598057746887 Acc:0.8633213043212891\n",
            "Train | 27 - Loss:0.5066598057746887 Acc:0.8633213043212891\n",
            "Valid | 27 - Loss:4.099691867828369 Acc:0.2907652258872986\n",
            "Time Taken:: 27: 169.70\n",
            "Model Saved at model_ckpt/vit/ckpt-27\n",
            " 27% 27/100 [1:17:37<3:27:36, 170.64s/it]\n",
            "\n",
            "Start Training 28\n",
            "\n",
            "\n",
            "Train | Step:: 10540 - Loss:0.4723617136478424 Acc:0.8828125\n",
            "Train | Step:: 10550 - Loss:0.4708285331726074 Acc:0.874218761920929\n",
            "Train | Step:: 10560 - Loss:0.470600962638855 Acc:0.8729166388511658\n",
            "Train | Step:: 10570 - Loss:0.46271786093711853 Acc:0.8755859136581421\n",
            "Train | Step:: 10580 - Loss:0.46703165769577026 Acc:0.8765624761581421\n",
            "Train | Step:: 10590 - Loss:0.4697662591934204 Acc:0.8768228888511658\n",
            "Train | Step:: 10600 - Loss:0.47097069025039673 Acc:0.8768973350524902\n",
            "Train | Step:: 10610 - Loss:0.4733906686306 Acc:0.875781238079071\n",
            "Train | Step:: 10620 - Loss:0.4666191041469574 Acc:0.8769097328186035\n",
            "Train | Step:: 10630 - Loss:0.4552196264266968 Acc:0.8795312643051147\n",
            "Train | Step:: 10640 - Loss:0.4514712393283844 Acc:0.8798295259475708\n",
            "Train | Step:: 10650 - Loss:0.4511418640613556 Acc:0.8792968988418579\n",
            "Train | Step:: 10660 - Loss:0.4513493478298187 Acc:0.8797475695610046\n",
            "Train | Step:: 10670 - Loss:0.45110005140304565 Acc:0.8801339268684387\n",
            "Train | Step:: 10680 - Loss:0.45637714862823486 Acc:0.87890625\n",
            "Train | Step:: 10690 - Loss:0.45942187309265137 Acc:0.878125011920929\n",
            "Train | Step:: 10700 - Loss:0.457506388425827 Acc:0.8784926533699036\n",
            "Train | Step:: 10710 - Loss:0.4616069495677948 Acc:0.8770833611488342\n",
            "Train | Step:: 10720 - Loss:0.46616363525390625 Acc:0.875698983669281\n",
            "Train | Step:: 10730 - Loss:0.4676744341850281 Acc:0.8746874928474426\n",
            "Train | Step:: 10740 - Loss:0.4682372212409973 Acc:0.8747767806053162\n",
            "Train | Step:: 10750 - Loss:0.46831631660461426 Acc:0.8749644756317139\n",
            "Train | Step:: 10760 - Loss:0.46490103006362915 Acc:0.8754076361656189\n",
            "Train | Step:: 10770 - Loss:0.4634515047073364 Acc:0.8759114742279053\n",
            "Train | Step:: 10780 - Loss:0.45818644762039185 Acc:0.8773437738418579\n",
            "Train | Step:: 10790 - Loss:0.4557996988296509 Acc:0.8777043223381042\n",
            "Train | Step:: 10800 - Loss:0.4527391791343689 Acc:0.8784722089767456\n",
            "Train | Step:: 10810 - Loss:0.4523243308067322 Acc:0.8785156011581421\n",
            "Train | Step:: 10820 - Loss:0.45037779211997986 Acc:0.8788254261016846\n",
            "Train | Step:: 10830 - Loss:0.44968393445014954 Acc:0.8790104389190674\n",
            "Train | Step:: 10840 - Loss:0.44802048802375793 Acc:0.8790574669837952\n",
            "Train | Step:: 10850 - Loss:0.44638457894325256 Acc:0.8796142339706421\n",
            "Train | Step:: 10860 - Loss:0.44718706607818604 Acc:0.8794507384300232\n",
            "Train | Step:: 10870 - Loss:0.44652971625328064 Acc:0.8798253536224365\n",
            "Train | Step:: 10880 - Loss:0.44390028715133667 Acc:0.8803125023841858\n",
            "Train | Step:: 10890 - Loss:0.44156643748283386 Acc:0.8811414837837219\n",
            "Train | Step:: 10900 - Loss:0.44128134846687317 Acc:0.8813555836677551\n",
            "Train | Step:: 10910 - Loss:0.4401057958602905 Acc:0.881640613079071\n",
            "Train | Step:: 10920 - Loss:0.4381670355796814 Acc:0.8821113705635071\n",
            "Train | 28 - Loss:0.4381670355796814 Acc:0.8821113705635071\n",
            "Valid | 28 - Loss:4.20065975189209 Acc:0.3031851053237915\n",
            "Time Taken:: 28: 169.73\n",
            "Model Saved at model_ckpt/vit/ckpt-28\n",
            " 28% 28/100 [1:20:27<3:24:46, 170.64s/it]\n",
            "\n",
            "Start Training 29\n",
            "\n",
            "\n",
            "Train | Step:: 10930 - Loss:0.39022761583328247 Acc:0.89453125\n",
            "Train | Step:: 10940 - Loss:0.407421737909317 Acc:0.8949218988418579\n",
            "Train | Step:: 10950 - Loss:0.4038580358028412 Acc:0.8927083611488342\n",
            "Train | Step:: 10960 - Loss:0.409832626581192 Acc:0.891406238079071\n",
            "Train | Step:: 10970 - Loss:0.40582045912742615 Acc:0.8909375071525574\n",
            "Train | Step:: 10980 - Loss:0.4076515734195709 Acc:0.8908854126930237\n",
            "Train | Step:: 10990 - Loss:0.409617155790329 Acc:0.8897321224212646\n",
            "Train | Step:: 11000 - Loss:0.4105287492275238 Acc:0.8896484375\n",
            "Train | Step:: 11010 - Loss:0.40857696533203125 Acc:0.8904513716697693\n",
            "Train | Step:: 11020 - Loss:0.4060521423816681 Acc:0.8904687762260437\n",
            "Train | Step:: 11030 - Loss:0.39975956082344055 Acc:0.8926136493682861\n",
            "Train | Step:: 11040 - Loss:0.3986630141735077 Acc:0.893359363079071\n",
            "Train | Step:: 11050 - Loss:0.40261271595954895 Acc:0.8921273946762085\n",
            "Train | Step:: 11060 - Loss:0.40295565128326416 Acc:0.8923549056053162\n",
            "Train | Step:: 11070 - Loss:0.40501993894577026 Acc:0.8916666507720947\n",
            "Train | Step:: 11080 - Loss:0.4087727963924408 Acc:0.890673816204071\n",
            "Train | Step:: 11090 - Loss:0.4076280891895294 Acc:0.890900731086731\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}